[{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1608899934,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/kautilya-tripathi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kautilya-tripathi/","section":"authors","summary":"","tags":null,"title":"Kautilya Tripathi","type":"authors"},{"authors":["Kautilya Tripathi"],"categories":["docker","tech"],"content":"Table of Contents  What is Seccomp? How docker uses Seccomp? Creating out own seccomp profile json file Conclusion Did you find this page helpful? Consider sharing it ðŸ™Œ   What is Seccomp? As per the kernel documentation\n A large number of system calls are exposed to every userland process with many of them going unused for the entire lifetime of the process. As system calls change and mature, bugs are found and eradicated. A certain subset of userland applications benefit by having a reduced set of available system calls. The resulting set reduces the total kernel surface exposed to the application. System call filtering is meant for use with those applications.\n  Seccomp filtering provides a means for a process to specify a filter for incoming system calls. The filter is expressed as a Berkeley Packet Filter (BPF) program, as with socket filters, except that the data operated on is related to the system call being made: system call number and the system call arguments. This allows for expressive filtering of system calls using a filter program language with a long history of being exposed to userland and a straightforward data set.\n How docker uses Seccomp? Secure computing mode (seccomp) is a Linux kernel feature. You can use it to restrict the actions available within the container. The seccomp() system call operates on the seccomp state of the calling process. You can use this feature to restrict your applicationâ€™s access.\nThis feature is available only if Docker has been built with seccomp and the kernel is configured with CONFIG_SECCOMP enabled. To check if your kernel supports seccomp:\n$ grep CONFIG_SECCOMP= /boot/config-$(uname -r) CONFIG_SECCOMP=y  The default seccomp profile provides a sane default for running containers with seccomp and disables around 44 system calls out of 300+. It is moderately protective while providing wide application compatibility. The default Docker profile can be found here.\nseccomp is instrumental for running Docker containers with least privilege. It is not recommended to change the default seccomp profile.\nWhen you run a container, it uses the default profile unless you override it with the --security-opt option. For example, the following explicitly specifies a policy:\ndocker run --rm \\ -it \\ --security-opt seccomp=/path/to/seccomp/profile.json \\ hello-world  Let\u0026rsquo;s take a look at snippet of syscalls allowed from the default profile:\n{ \u0026quot;names\u0026quot;: [ \u0026quot;bpf\u0026quot;, \u0026quot;clone\u0026quot;, \u0026quot;fanotify_init\u0026quot;, \u0026quot;lookup_dcookie\u0026quot;, \u0026quot;mount\u0026quot;, \u0026quot;name_to_handle_at\u0026quot;, \u0026quot;perf_event_open\u0026quot;, \u0026quot;quotactl\u0026quot;, \u0026quot;setdomainname\u0026quot;, \u0026quot;sethostname\u0026quot;, \u0026quot;setns\u0026quot;, \u0026quot;syslog\u0026quot;, \u0026quot;umount\u0026quot;, \u0026quot;umount2\u0026quot;, \u0026quot;unshare\u0026quot; ], \u0026quot;action\u0026quot;: \u0026quot;SCMP_ACT_ALLOW\u0026quot;, \u0026quot;args\u0026quot;: [], \u0026quot;comment\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;includes\u0026quot;: { \u0026quot;caps\u0026quot;: [ \u0026quot;CAP_SYS_ADMIN\u0026quot; ] }, \u0026quot;excludes\u0026quot;: {} }  names field in above json snippet refers to syscalls of linux kernel. They are only allowed for containers that you run with capability CAP_SYS_ADMIN mentioned in action json field. You can pass this capability to a container using --cap-add flag.\nCreating out own seccomp profile json file One thing to know is that every executable binary in unix system has some capabilities assigned to it. For example if you want to find capabilities assined to ping binary just use getcap command like this:\n$ getcap $(which ping) /usr/bin/ping = cap_net_admin,cap_net_raw+p  So what we will do that, is we will be tying CAP_AUDIT_CONTROL capability to our chown syscall. You can take any other capability other than CAP_CHOWN.\n This is purely for experimenting and understanding how seccomp profile will work. DO NOT use it in production environment.\n To make this work we will remove all the occurances of chown syscall from default seccomp profile and move it to our custom profile like this:\n{ \u0026quot;names\u0026quot;: [ \u0026quot;chown\u0026quot;, \u0026quot;chown32\u0026quot;, \u0026quot;fchown\u0026quot;, \u0026quot;fchown32\u0026quot;, \u0026quot;fchownat\u0026quot;, \u0026quot;lchown\u0026quot;, \u0026quot;lchown32\u0026quot; ], \u0026quot;action\u0026quot;: \u0026quot;SCMP_ACT_ALLOW\u0026quot;, \u0026quot;args\u0026quot;: [], \u0026quot;comment\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;includes\u0026quot;: { \u0026quot;caps\u0026quot;: [ \u0026quot;CAP_AUDIT_CONTROL\u0026quot; ] }, \u0026quot;excludes\u0026quot;: {} }  Final json file can be found here. Copy it from github gist and save it as custom-profile.json because it will be used in our next step for running docker container. Run the below command:\n$ docker run --rm -it --security-opt seccomp=custom-profile.json debian bash # Try creating a user root@429a518f8ec5:/# useradd knrt10 useradd: failure while writing changes to /etc/passwd  Above command will fail as useradd syscall uses CAP_CHOWN internally. That is different topic, I will write an article about it another time.\nNow exit and try to run the docker container using this command:\n$ docker run --cap-add=CAP_AUDIT_CONTROL --rm -it --security-opt seccomp=custom-profile.json debian bash # create a new user root@ea95510fcb7c:/# useradd -m knrt10 # check current user i.e root root@ea95510fcb7c:/# id -u 0 # create a file root@ea95510fcb7c:/# touch a # check permissions on file. Note currently it is owned by root root@ea95510fcb7c:/# ls -l a -rw-r--r-- 1 root root 0 Jan 30 11:21 a # change ownership to earlier created user root@ea95510fcb7c:/# chown knrt10 a # check permissions on file. It is changed to user knrt10 root@ea95510fcb7c:/# ls -l a -rw-r--r-- 1 knrt10 root 0 Jan 30 11:21 a  When running the docker container by explicitly specifying the capability CAP_AUDIT_CONTROL and then the container allows and uses syscall chown. In this way you can create your own profile and tie it up with any capability you want.\nConclusion You learnt how seccomp profiles are used by docker and how you can create a custom seccomp profile and use it while running your docker container. This is mostly used for security purposes when you don\u0026rsquo;t want your container to have extra kernel priviledges. You can learn more in details from docker documentation.\nDid you find this page helpful? Consider sharing it ðŸ™Œ ","date":1611964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611964800,"objectID":"1eb9a51f31e61a3bb02c3c895de277fb","permalink":"/post/seccomp-security-profiles-docker/","publishdate":"2021-01-30T00:00:00Z","relpermalink":"/post/seccomp-security-profiles-docker/","section":"post","summary":"Understand Seccomp profiles for Docker while experimenting and creating our own seccomp profile json file and running docker container with it.","tags":["docker","security"],"title":"Seccomp security profiles for Docker","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["golang","tech"],"content":"Table of Contents  About What are structs in Golang? Declaring a struct Using a Struct Memory Representation Memory Optimisation Conclusion Did you find this page helpful? Consider sharing it ðŸ™Œ   About In the previous post we learnt about Macro view of Maps in Golang, but in this post we will learn more about Structs. Structs are building blocks of a Go application and anyone writing modular Go code will find themselves using it a lot. This spurred me to take a deep dive into exactly what a struct is, how they are represented in memory, and how to get the most out of structs.\nWhat are structs in Golang? A struct is a user-defined type that represents a collection of fields. It can be used in places where it makes sense to group the data into a single unit rather than having each of them as separate values.\nIn simple language, an employee has a firstName, lastName and age. It makes sense to group these three properties into a single struct named Employee.\ntype Employee struct { firstName string lastName string age int }  Declaring a struct type Employee struct { firstName string lastName string age int }  The above snippet declares a struct type Employee with fields firstName, lastName and age. The above Employee struct is called a named struct because it creates a new data type named Employee using which Employee structs can be created.\nThis struct can also be made more compact by declaring fields that belong to the same type in a single line followed by the type name. In the above struct firstName and lastName belong to the same type string and hence the struct can be rewritten as\ntype Employee struct { firstName, lastName string age int }   Although the above syntax saves a few lines of code, it doesn\u0026rsquo;t make the field declarations explicit. Please refrain from using the above syntax.\n Using a Struct package main import ( \u0026quot;fmt\u0026quot; ) type Employee struct { firstName string lastName string age int salary int } func main() { //creating struct specifying field names. emp1 := Employee{ firstName: \u0026quot;Sam\u0026quot;, age: 25, salary: 500, lastName: \u0026quot;Anderson\u0026quot;, } //creating struct without specifying field names. emp2 := Employee{\u0026quot;Thomas\u0026quot;, \u0026quot;Paul\u0026quot;, 29, 800} fmt.Println(\u0026quot;Employee 1\u0026quot;, emp1) fmt.Println(\u0026quot;Employee 2\u0026quot;, emp2) }  In the above code we first define a struct Employee with the given fields. Then inside the main function we declare a variable emp1 of type Employee with it\u0026rsquo;s corresponding values. Any fields omitted from when instantiating a struct will take on the zero value of that field\u0026rsquo;s type. E.g. if age was omitted when creating a user the default value is 0.\nSimilarly we define a variable emp2 with diffrent values and then print both the structs. The above program outputs\nEmployee 1 {Sam Anderson 25 500} Employee 2 {Thomas Paul 29 800}  This was the basics of structs, to learn more about struct follow the tour of go. This guide focuses more on memory representation of struct so we will now learn about it.\nMemory Representation When it comes to memory allocation for structs, they are always allocated contiguous, byte-aligned blocks of memory, and fields are allocated and stored in the order that they are defined.\nThe concept of byte-alignment in this context means that the contiguous blocks of memory are aligned at offsets equal to the platforms word size (4 bytes on a 32-bit, 8 bytes on a 64-bit system). Consider the following example of a struct where there are three fields each of varying sizes, on a 64-bit environment blocks of memory will be aligned at 8 byte offsets.\nThis results in the first block of 8 bytes being fully occupied by a (8 bytes). The next block of memory (offset by 8 bytes from the starting memory address of the struct) has its first 2 bytes occupied by b, the next 1 byte occupied by c then the remaining 5 bytes are empty pad bytes.\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;unsafe\u0026quot; ) type MemoryTaken struct { a int // 8 byte b int16 // 2 bytes c bool // 1 bytes } func main() { fmt.Printf(\u0026quot;Sizeof MemoryTaken: %d\\n\u0026quot;, unsafe.Sizeof(MemoryTaken{})) }  When you run the above program you will get the following output:\nSizeof MemoryTaken: 16  Let us see the memory struct layout for the above struct MemoryTaken\n  Memory Struct Layout   Memory Optimisation Considering how memory is allocated for structs as seen in the previous section, depending on the order that fields are defined in a struct it can be rather inefficient due to the number of pad bytes required. It is possible to optimise the memory utilisation of a struct however, by defining the fields in a deliberate order to maximise the use of each block of memory, reducing the need for redundant pad bytes.\nThe following example there is a struct Canditate representing a candidate for a company. In the first iteration, before taking steps to optimise its memory utilisation the total memory of the combined fields totals 35 bytes, however, the total struct size equates to 48 bytes due to pad bytes.\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;unsafe\u0026quot; ) type Canditate struct { hired bool // 1 byte name string // 16 Bytes position string // 16 Bytes age int16 // 2 bytes // 35 bytes total, 13 bytes padding } func main() { fmt.Printf(\u0026quot;Sizeof Unoptimized Canditate struct: %d\\n\u0026quot;, unsafe.Sizeof(Canditate{})) }  When you run the above program you will get the following output:\nSizeof Unoptimized Canditate struct: 48  Now we will see the memory representation for the struct Canditate above:\n  Unoptimized Candidate Struct   Now, if the struct fields are re-arranged to minimise padding bytes, the resulting struct size is only 40 bytes.\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;unsafe\u0026quot; ) type Canditate struct { name string // 16 Bytes position string // 16 Bytes age int16 // 2 bytes hired bool // 1 byte // 35 bytes total, 5 bytes padding } func main() { fmt.Printf(\u0026quot;Sizeof Optimized Canditate struct: %d\\n\u0026quot;, unsafe.Sizeof(Canditate{})) }  When you run the above program you will get the following output:\nSizeof Optimized Canditate struct: 40    Optimized Candidate Struct   Conclusion In modern systems where memory constraints are not typically an issue, the benefit gained from micro-optimisations like this, reclaiming 8 bytes of memory is not enormous. However, the ability to understand at this level, how a struct is allocated memory, and how to, if required apply such optimisations is invaluable.\nDid you find this page helpful? Consider sharing it ðŸ™Œ ","date":1611360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611402085,"objectID":"631b255f4a1f6845d584692618222825","permalink":"/post/golang-struct-anatomy/","publishdate":"2021-01-23T00:00:00Z","relpermalink":"/post/golang-struct-anatomy/","section":"post","summary":"Deep dive into golang structs and how they are represented in memory and learn about memory optimization.","tags":["golang","internals","hard"],"title":"Anatomy of Structs in Golang","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["golang","tech"],"content":"Table of Contents  About What are maps in Golang? Introduction Creating and Using Maps Macro view How Maps Grow Conclusion Did you find this page helpful? Consider sharing it ðŸ™Œ   About This article will focus on explaining macro view of maps in Go and why are they an unsorted collections. You should have atleat a basic knowledge of what are maps in Golang. If you don\u0026rsquo;t know don\u0026rsquo;t worry I will try to explain it as clearly as I can.\nWhat are maps in Golang? A map is a builtin type in Go that is used to store key-value pairs. Let\u0026rsquo;s take the example of a startup with a few employees. For simplicity, let\u0026rsquo;s assume that the first name of all these employees is unique. We are looking for a data structure to store the salary of each employee. A map will be a perfect fit for this use case. The name of the employee can be the key and the salary can be the value. Maps are similar to dictionaries in other languages such as Python.\nTo learn more about map, refer to golang website.\nIntroduction There are lots of posts that talk about the internals of slices, but when it comes to maps, we are left in the dark. I was wondering why and then I found the code for maps and it all made sense. At least for me, this code is complicated. That being said, I think we can create a macro view of how maps are structured and grow. This should explain why they are unordered, efficient and fast.\nCreating and Using Maps Letâ€™s look at how we can use a map literal to create a map and store a few values:\n// Create an empty map with a key and value of type string. employee := map[string]string{} // Add a few keys/value pairs to the map. employee[\u0026quot;name\u0026quot;] = \u0026quot;knrt10\u0026quot; employee[\u0026quot;age\u0026quot;] = \u0026quot;23\u0026quot; employee[\u0026quot;tag\u0026quot;] = \u0026quot;Cloud Infrastructure Engineer\u0026quot; employee[\u0026quot;location\u0026quot;] = \u0026quot;India\u0026quot;  When we add values to a map, we always specify a key that is associated with the value. This key is used to find this value again without the need to iterate through the entire collection:\nfmt.Printf(\u0026quot;Value: %s\u0026quot;, employee[\u0026quot;name\u0026quot;])  If we do iterate through the map, we will not necessarily get the keys back in the same order. In fact, every time you run the code, the order could change:\nemployee := map[string]string{} employee[\u0026quot;name\u0026quot;] = \u0026quot;knrt10\u0026quot; employee[\u0026quot;age\u0026quot;] = \u0026quot;23\u0026quot; employee[\u0026quot;tag\u0026quot;] = \u0026quot;Cloud Infrastructure Engineer\u0026quot; employee[\u0026quot;location\u0026quot;] = \u0026quot;India\u0026quot; for key, value := range employee { fmt.Printf(\u0026quot;%s:%s, \u0026quot;, key, value) } # Output: location:India, name:knrt10, age:23, tag:Cloud Infrastructure Engineer, name:knrt10, age:23, tag:Cloud Infrastructure Engineer, location:India,  Now that we know how to create, set key/value pairs and iterate over a map, we can peek under the hood.\nMacro view Maps in Go are implemented as a hash table. If you need to learn what a hash table is, there are lots of articles and posts about the subject. You can learn about Hash table on Wikipedia.\nA map is just a hash table. The data is arranged into an array of buckets. The number of buckets is always equal to a power of 2. Each bucket contains up to 8 key/value pairs.\nWhen a map operation is performed, such as (employee[\u0026quot;name\u0026quot;] = \u0026quot;knrt10\u0026quot;), a hash key is generated against the key that is specified. In this case the string \u0026ldquo;name\u0026rdquo; is used to generate the hash key. The low order bits (LOB) of the generated hash key is used to select a bucket. If more than 8 keys hash to a bucket, it chains on extra buckets.\nThe low-order bits of the hash are used to select a bucket. Each bucket contains a few high-order bits of each hash to distinguish the entries within a single bucket.\n  Buckets in Map   Once a bucket is selected, the key/value pair needs to be stored, removed or looked up, depending on the type of operation. If we look inside any bucket, we will find two data structures. First, there is an array with the top 8 high order bits (HOB) from the same hash key that was used to select the bucket. This array distinguishes each individual key/value pair stored in the respective bucket. Second, there is an array of bytes that store the key/value pairs. The byte array packs all the keys and then all the values together for the respective bucket.\nWhen we are iterating through a map, the iterator walks through the array of buckets and then return the key/value pairs in the order they are laid out in the byte array. This is why maps are unsorted collections. The hash keys determines the walk order of the map because they determine which buckets each key/value pair will end up in.\nThe 1 byte value in this map would result in 7 extra bytes of padding per key/value pair. By packing the key/value pairs as key/key/value/value, the padding only has to be appended to the end of the byte array and not in between. Eliminating the padding bytes saves the bucket and the map a good amount of memory. I will explain about alignment boundaries is some other article\nA bucket is configured to store only 8 key/value pairs. If a ninth key needs to be added to a bucket that is full, an overflow bucket is created and reference from inside the respective bucket.\n  Buckets Resizing Map   How Maps Grow As we continue to add or remove key/value pairs from the map, the efficiency of the map lookups begin to deteriorate. The load threshold values that determine when to grow the hash table are based on these four factors:\n% overflow : Percentage of buckets which have an overflow bucket bytes/entry : Number of overhead bytes used per key/value pair hitprobe : Number of entries that need to be checked when looking up a present key missprobe : Number of entries that need to be checked when looking up an absent key  Some stats for different loads for (64-bit, 8 byte keys and elems)\n// loadFactor %overflow bytes/entry hitprobe missprobe // 4.00 2.13 20.77 3.00 4.00 // 4.50 4.05 17.30 3.25 4.50 // 5.00 6.85 14.77 3.50 5.00 // 5.50 10.55 12.94 3.75 5.50 // 6.00 15.27 11.67 4.00 6.00 // 6.50 20.90 10.79 4.25 6.50 // 7.00 27.14 10.15 4.50 7.00 // 7.50 34.03 9.73 4.75 7.50 // 8.00 41.10 9.40 5.00 8.00  Growing the hash table starts with assigning a pointer called the \u0026ldquo;old bucket\u0026rdquo; pointer to the current bucket array. Then a new bucket array is allocated to hold twice the number of existing buckets. This could result in large allocations, but the memory is not initialized so the allocation is fast.\nImportant: Once the memory for the new bucket array is available, the key/value pairs from the old bucket array can be moved or \u0026ldquo;evacuated\u0026rdquo; to the new bucket array. Evacuations happen as key/value pairs are added or removed from the map. The key/value pairs that are together in an old bucket could be moved to different buckets inside the new bucket array. The evacuation algorithm attempts to distribute the key/value pairs evenly across the new bucket array.\nConclusion This is just a macro view of how maps are structured and grown. You can look into the the code to undestand more how it works. It does show that if you know how many keys you need ahead of time, it is best to allocated that space during initialization.\nDid you find this page helpful? Consider sharing it ðŸ™Œ ","date":1610150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611402085,"objectID":"d0c10e36192e5c13d71aeb2096c2fbfc","permalink":"/post/golang-maps-internals/","publishdate":"2021-01-09T00:00:00Z","relpermalink":"/post/golang-maps-internals/","section":"post","summary":"Understand how maps are stored in Golang internally and why are they an unsorted collections.","tags":["golang","internals","hard"],"title":"Macro view of Maps in Golang","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["kubernetes","tech"],"content":"Table of Contents  What we will learn What is Pod Security Policy?  Pod Security Policies allow you to control:   Enabling Pod Security Policies  Policy order   Creating Custom Kubernetes Pod Security Policy  Set up Create a policy Grant access to Service Account Create a pod using custom psp Pro Tip   Did you find this page helpful? Consider sharing it ðŸ™Œ   What we will learn  What is a PSP? How to enable and use PSP in a cluster(minikube) Create custom psp and use it in a pod  What is Pod Security Policy? Pod Security Policies are cluster-wide resources that control security sensitive aspects of pod specification. PSP objects define a set of conditions that a pod must run with in order to be accepted into the system, as well as defaults for their related fields. PodSecurityPolicy is an optional admission controller that is enabled by default through the API, thus policies can be deployed without the PSP admission plugin enabled. This functions as a validating and mutating controller simultaneously.\nPod Security Policies allow you to control:  The running of privileged containers Usage of host namespaces Usage of host networking and ports Usage of volume types Usage of the host filesystem A white list of Flexvolume drivers The allocation of an FSGroup that owns the podâ€™s volumes Requirments for use of a read only root file system The user and group IDs of the container Escalations of root privileges Linux capabilities, SELinux context, AppArmor, seccomp, sysctl profile  If youâ€™re interested in more details, check out the official Kubernetes documentation.\nEnabling Pod Security Policies For learning purpose we will be testing on our minikube cluster.\nEnable PodSecurityPolicy on your minikube cluster by appening PodSecurityPolicy to the apiserver flag in minikube like this:\n--extra-config=apiserver.enable-admission-plugins=PodSecurityPolicy  Important: As per kubernetes documentation\n Pod security policy control is implemented as an optional (but recommended) admission controller. PodSecurityPolicies are enforced by enabling the admission controller, but doing so without authorizing any policies will prevent any pods from being created in the cluster. Since the pod security policy API (policy/v1beta1/podsecuritypolicy) is enabled independently of the admission controller, for existing clusters it is recommended that policies are added and authorized before enabling the admission controller.\n This means if your start your cluster without adding and authorizing policies it will fail to start any new pod. To see this in working, follow below steps:\n# Start your minikube cluster without any policies defined minikube start -p kautilya-cluster --kubernetes-version=v1.19.6 --feature-gates=EphemeralContainers=true --extra-config=apiserver.enable-admission-plugins=PodSecurityPolicy --addons=pod-security-policy # Try running a simple pod kubectl run nginx --image=nginx  The above command will fail with the follwing output:\nError from server (Forbidden): pods \u0026quot;nginx\u0026quot; is forbidden: PodSecurityPolicy: no providers available to validate pod request\nTo fix the above problem you have to create a psp.yaml file inside ~/.minikube/files/etc/kubernetes/addons folder. This will fix the issue when the cluster is being bootstraped. Copy the contents of file below and run the command:\nmkdir -p ~/.minikube/files/etc/kubernetes/addons \u0026amp;\u0026amp; \\ cat \u0026lt;\u0026lt;EOF | tee ~/.minikube/files/etc/kubernetes/addons/psp.yaml | kubectl apply -f - --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: privileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: \u0026quot;*\u0026quot; labels: addonmanager.kubernetes.io/mode: EnsureExists spec: privileged: true allowPrivilegeEscalation: true allowedCapabilities: - \u0026quot;*\u0026quot; volumes: - \u0026quot;*\u0026quot; hostNetwork: true hostPorts: - min: 0 max: 65535 hostIPC: true hostPID: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny' --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted labels: addonmanager.kubernetes.io/mode: EnsureExists spec: privileged: false allowPrivilegeEscalation: false requiredDropCapabilities: - ALL volumes: - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' - 'persistentVolumeClaim' hostNetwork: false hostIPC: false hostPID: false runAsUser: rule: 'MustRunAsNonRoot' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 readOnlyRootFilesystem: false --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: psp:privileged labels: addonmanager.kubernetes.io/mode: EnsureExists rules: - apiGroups: ['policy'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: - privileged --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: psp:restricted labels: addonmanager.kubernetes.io/mode: EnsureExists rules: - apiGroups: ['policy'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: - restricted --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default:restricted labels: addonmanager.kubernetes.io/mode: EnsureExists roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: psp:restricted subjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default:privileged namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExists roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: psp:privileged subjects: - kind: Group name: system:masters apiGroup: rbac.authorization.k8s.io - kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.io - kind: Group name: system:serviceaccounts:kube-system apiGroup: rbac.authorization.k8s.io EOF  Let\u0026rsquo;s check the PSPs created:\nkubectl get psp NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES privileged true * RunAsAny RunAsAny RunAsAny RunAsAny false * restricted false RunAsAny MustRunAsNonRoot MustRunAs MustRunAs false configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim  Now if you try to create a pod, it will be created successfully.\nkubectl run nginx --image=nginx pod/nginx created  Note: One thing you need to see which psp is assigned to the pod. Run the command below:\nkubectl describe po nginx | grep kubernetes.io/psp Annotations: kubernetes.io/psp: privileged  As you can see that privileged psp is assigned to it. Buy why privileged? That\u0026rsquo;s where policy order come in action.\nPolicy order As per kubernetes documentation, it says:\nIn addition to restricting pod creation and update, pod security policies can also be used to provide default values for many of the fields that it controls. When multiple policies are available, the pod security policy controller selects policies according to the following criteria:\n  PodSecurityPolicies which allow the pod as-is, without changing defaults or mutating the pod, are preferred. The order of these non-mutating PodSecurityPolicies doesn\u0026rsquo;t matter.\n  If the pod must be defaulted or mutated, the first PodSecurityPolicy (ordered by name) to allow the pod is selected.\n   Understanding this policy order is a black hole. More your experiment more you go into the black hole. So one thing you need to keep in mind is that you need to match the po.spec and psp.spec and if more than 2 policies match the criterion, the policy which comes first will be selected. To understand more, we will try to create a custom policy and run it in our pod.\n Creating Custom Kubernetes Pod Security Policy Set up Set up a namespace and a service account to act as for this example. We\u0026rsquo;ll use this service account which will have access to our custom psp and use this inside our pod.\nkubectl create namespace testing-psp kubectl create serviceaccount -n testing-psp testing-psp  Create a policy Define the minimal-psp-restricted PodSecurityPolicy object. This is a policy that simply prevents the creation of privileged pods, and run user as non root. The name of a PodSecurityPolicy object must be a valid DNS subdomain name. Run the below command:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: minimal-psp-restricted spec: privileged: false # Don't allow privileged pods! # The rest fills in some required fields. seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny runAsUser: rule: MustRunAsNonRoot fsGroup: rule: RunAsAny volumes: - '*' EOF  Check the psp created:\nkubectl get psp NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES minimal-psp-restricted false RunAsAny MustRunAsNonRoot RunAsAny RunAsAny false * privileged true * RunAsAny RunAsAny RunAsAny RunAsAny false * restricted false RunAsAny MustRunAsNonRoot MustRunAs MustRunAs false configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim  Grant access to Service Account Our Service Account testing-psp should have permission to use the new policy. To check whether it has access run the command:\nkubectl auth can-i use podsecuritypolicy/minimal-psp-restricted --as=\u0026quot;system:serviceaccount:testing-psp:testing-psp\u0026quot; no  Create the rolebinding to grant service account testing-psp the use verb on the minimal-psp-restricted policy:\n# Create a role kubectl create role psp:minimalunprivileged \\ --verb=use \\ --resource=podsecuritypolicy \\ --resource-name=minimal-psp-restricted \\ --namespace=testing-psp role.rbac.authorization.k8s.io/psp:minimalunprivileged created # Create a rolebinding kubectl create rolebinding psp:minimalunprivilegedbinding \\ --role=psp:minimalunprivileged \\ --serviceaccount=testing-psp:testing-psp \\ --namespace=testing-psp rolebinding.rbac.authorization.k8s.io/psp:minimalunprivilegedbinding created # Check access to use psp kubectl auth can-i use podsecuritypolicy/minimal-psp-restricted --as=\u0026quot;system:serviceaccount:testing-psp:testing-psp\u0026quot; yes  Create a pod using custom psp Let us create a pod using our service account. Run the file below:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: labels: run: nginx name: nginx spec: serviceAccountName: testing-psp containers: - image: nginx name: nginx resources: {} securityContext: runAsNonRoot: true # container should run as roon root runAsUser: 65534 # this means running as nobody runAsGroup: 65534 # this means running as nogroup dnsPolicy: ClusterFirst restartPolicy: Always status: {} EOF  Once the pod is created, let us check which psp is assigned to it\nkubectl describe po nginx | grep kubernetes.io/psp Annotations: kubernetes.io/psp: minimal-psp-restricted  As you can see that it is using our custom psp minimal-psp-restricted because po.spec for our nginx pod matches psp.spec fields from minimal-psp-restricted policy.\nThatâ€™s it. Youâ€™ve created and applied your first Kubernetes pod security policy. With the help of this technique, you can greatly enhance the security of your Kubernetes deployments.\nPro Tip When creating a custom policy, always start with privileged psp and then start changing fields for po.spec according to changes in your policy. This can help you in debugging.\nDid you find this page helpful? Consider sharing it ðŸ™Œ ","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609575180,"objectID":"e1b033d760ff75cbf090cb91840f4bc7","permalink":"/post/psp/","publishdate":"2021-01-02T00:00:00Z","relpermalink":"/post/psp/","section":"post","summary":"Beginner's guide for learning PSP in a kubernetes cluster.","tags":["kubernetes","security"],"title":"Learning about Pod Security Policy","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["kubernetes","tech"],"content":"Table of Contents  How it started How I practiced Explaining the whole process What\u0026rsquo;s in the exam? Basic Understanding of systemd Networking Linux Commands k8s Imperative Commands k8s Components Practice Browser Terminal Setup  Minimal Setup   Be fast CKA Preparation  Read the Curriculum Read the Handbook Read the important tips Read the FAQ   Conclusion Did you find this page helpful? Consider sharing it ðŸ™Œ   How it started After doing a good amount of internships I started my first job working as Cloud Infrastructure Engineer at Kinvolk. Initially when I started the job I had little experience with Kubernetes, I had only worked on setting up cluster on GKE or worked on pre-setup clusters like minikube. When I started the job I worked on our product Lokomotive which is an open source Kubernetes distribution that ships pure upstream Kubernetes. It focuses on being minimal, easy to use, and secure by default. While working on that I got into deeper details of Kubernetes Infrastructure. Since then I have been trying to learn as much as I can into greater detail.\nHow I practiced  I did not took any course or followed any professional training. I mostly prefer going though documentation and practicing it. Rest it\u0026rsquo;s up to you how you proceed. My day to day work is regarging k8s only but I did practice for 2 weeks for CKA exam to solve questions in time limit.\n I have a Mackbook air 2017 model with 8gb RAM. So why am I telling this?\nIt\u0026rsquo;s because my laptop has gone to dust, it was hard for me to practice on my laptop. My situation currently is if I start a minikube cluster and open vscode on side, it takes some 20-30 seconds for just a file to save. Initially when my laptop somehow did not lag, I used to spun up Vagrant ubuntu machine to practice but that idea went to drain once I laptop started lagging more. To fix this problem what I did was that I used to spun up t1.small.x86 machine on Equinix Metal(Formerly Packet) and practice there.\nExplaining the whole process  You first buy the exam anytime from official CNCF website. After that you have 1 year to take the exam before it expires You then schedule the exam within 1 year of time frame after you bought it You then give the exam online while been proctored. Before beginning of the exam the proctor checks all the criteria is met as per rules and after that only exam is started You complete the exam You get result after 36-48 hours of exam  What\u0026rsquo;s in the exam? The exam consists of 15-20 questions, you are supposed to complete these questions in 2 hours and you have to score 66% or more in order to pass the exam. You are allowed to open only two browser tabs, one will be the exam interface and in the other tab you can open any of the allowed web pages, one of them is k8s official docs. You can get a list of all the link that you can visit, in the candidates handbook and it says you can access https://kubernetes.io/docs/, https://github.com/kubernetes/ and https://kubernetes.io/blog\nCandidateâ€™s handbook says that you can copy 1â€“2 lines of data from the official documentation and thatâ€™s sufficient, you wont actually need to copy entire yaml file into your test. Its better to generate that yaml file by yourself and then edit that yaml file to have specific details.\nAs most of you are aware the CKA exam requires a considerable amount of preparation since it focuses on practical/hands-on questions/scenarios rather than a set of Mutiple choice questions.\nBasic Understanding of systemd There are chances that some of the component of the cluster will be running as linux services and not as k8s pods, and you might have to debug those components to check any issues in them. So it will always be a plus to have good understanding of how to change configuration of, check logs(journalctl) of, start or stop a systemd service. You can easily learn from their official documentation.\nNetworking One of the most important thing in any distributed systems is Networking. I went little deep into it and learnt and practiced about:\n Switch routing(interfaces, routes, gateways) DNS Network Namespaces Docker Networking CNI Cluster Networking Pod and Service Networking  You should also be aware of some linux networking commands for example nslookup, ping, curl or dig in order to check the connectivity between the hosts or components.\nThis becomes very handy, if you have questions where you have to check the connectivity between the services or pods.\nLinux Commands You will be working on linux based machines so itâ€™s always beneficial to know some basic command of linux based operating systems. For example how to redirect output to a file (\u0026gt;), filter somethings from a file (grep), find, get last or first rows from the output and cut command. Practice awk command and you should also know how processes work.\nk8s Imperative Commands When you are asked to, letâ€™s say create a pod, itâ€™s not very ideal to write the entire manifests manually because its time consuming and you are likely to make some indention mistakes while writing the yaml. So its better to have the basic manifest of the resource generated and then edit that manifest with what is actually required by the question. More information can be found in k8s docs.\nk8s Components   The other part is understanding Kubernetes components and being able to fix and investigate clusters. Understand this: https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster\n  When you have to fix a component (like kubelet) in one cluster, just check how its setup on another node in the same or even another cluster. You can copy config files over etc\n  I suggest you do Kubernetes The Hard Way once. It\u0026rsquo;s not necessary to do it more often or know it all by heart, the CKA is not that complex. But KTHW helps understanding the concepts\n  It can help if you install your own cluster using kubeadm in a VM or using a cloud provider and investigate the components. I did it myself in a vagrant setup\n  Know how to use kubeadm to for example add nodes to a cluster\n  Know how to create an Ingress resources\n  Know how to snapshot/restore ETCD from another machine\n  Know advanced scheduling: https://kubernetes.io/docs/concepts/scheduling/kube-scheduler\n  Practice Practice is key to everything, the exam is totally based on questions that you will have to perform on live cluster. So until and unless you have actually done those things on live cluster before it will be hard for you but if you have done some practice/hands on you donâ€™t have to worry much about this.\nThe best way to practice k8s concepts is to get minikube installed and go through the tasks page of the official k8s docs. Once you have gone through the official k8s docs tasks you will be confident enough to appear in the exam.\nThere is no easy way out with the preparation practice, practice and sheer practice is the key to success here. Also a very economical usage of time and a personal strategy on how to target each question would greatly help crack this exam.\nBrowser Terminal Setup It should be considered to spend ~1 minute in the beginning to setup your terminal. In the real exam the vast majority of questions will be done from the main terminal. For few you might need to ssh into another machine. Just be aware that configurations to your shell will not be transferred in this case.\nMinimal Setup Alias You might have read in most of the articles to set up aliases for the exam. But personally I find it useless. I would suggest to minimally setup this alias:\necho 'alias kc=\u0026quot;kubectl\u0026quot;' \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc \u0026amp;\u0026amp; echo \u0026quot;source \u0026lt;(kubectl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; complete -F __start_kubectl kc  Vim Be great with vim.\ntoggle vim line numbers When in vim you can press Esc and type :set number or :set nonumber followed by Enter to toggle line numbers. This can be useful when finding syntax errors based on line - but can be bad when wanting to mark\u0026amp;copy by mouse. You can also just jump to a line number with Esc :22 + Enter.\nIndent multiple lines To indent multiple lines press Esc and type :set shiftwidth=2. First mark multiple lines using Shift v and the up/down keys. Then to indent the marked lines press \u0026gt; or \u0026lt;. You can then press . to repeat the action.\nAlso optionally you can create the file ~/.vimrc with the following content:\nset tabstop=2 set expandtab set shiftwidth=2  The expandtab make sure to use spaces for tabs. Memorize these and just type them down. You can\u0026rsquo;t have any written notes with commands on your desktop etc.\nBe fast Use the history command to reuse already entered commands or use even faster history search through Ctrl r .\nIf a command takes some time to execute, like sometimes kubectl delete pod x. You can put a task in the background using Ctrl z and pull it back into foreground running command fg or run bg to run in background while you perform other tasks.\nYou can delete pods fast with:\nkc delete pod x --grace-period 0 --force  CKA Preparation Read the Curriculum https://github.com/cncf/curriculum\nRead the Handbook https://docs.linuxfoundation.org/tc-docs/certification/lf-candidate-handbook\nRead the important tips https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad\nRead the FAQ https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad\nConclusion I would say it\u0026rsquo;s pretty easy if you have practiced and have good understanding how k8s works and how it is setup. It would help you while debugging and seting up cluster. Try solving questions with most weightage first. Relax and do not feel pressure about the time. Also you should keep in mind that CKA is just a tag. Kubernetes is very vast, so you should try to learn it as much as you can if you are interested in working in distributed systems domain.\nDid you find this page helpful? Consider sharing it ðŸ™Œ ","date":1608854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609063026,"objectID":"156b18404bd7133ac12d97f6e407cbd3","permalink":"/post/cka/","publishdate":"2020-12-25T00:00:00Z","relpermalink":"/post/cka/","section":"post","summary":"How I passed my Certified Kubenernets Administrator(CKA) exam.","tags":["kubernetes","cka","docker","security"],"title":"Passing CKA exam","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["opensource","tech"],"content":"So here I am writing my second article after some good and experiencing summer. This article will mainly focus on my experience how I did 3 internships at same time.\n  Open Mainframe Project(OMP):- 8 students were selected from all over the globe. This is really an aweome opportunity for college students, so if you don\u0026rsquo;t know about it try to apply in it next time. This is a remote internship.\n  Google Summer of Code(GSoC):- This is quite known in Indian community, so I don\u0026rsquo;t think I need to explain it more. This is a remote internship\n  Atlan:- This is a Delhi based startup, I was relocated to Delhi to work in their office.\n  It was around Febuarary and I was just contributing to open source projects as ususal and along with that I was working remotely for Atlan. Then GSoC was announced and I started to focus on GSoC org too, in which I decided to apply. Now let\u0026rsquo;s skip this part and focus on main stuff. I came to know about OMP from my best friend and colleague Palash Nigam. I was not even going to apply because I thought that I would never get selected. Now let me tell me you something. If you ever think about yourself like this, just stop. I was about to make the same mistake but I did not and I am reaping it\u0026rsquo;s rewards today. Always belive in yourself and always except the unexpected. So anyway after my friend gave me some lovely abusive words and to be precise he said (\u0026quot;Tum chutiye ho, kar lo apply\u0026quot;). I read some of the OMP projects and applied in two of them. One thing I want to say to freshers out there is that even if you think that your chances are low to get in anything don\u0026rsquo;t ever give your less than 100%. Always give your best because you won\u0026rsquo;t regret that you didn\u0026rsquo;t give your best. Even if you fail, it won\u0026rsquo;t matter because you tried and that is most important.\nNow at that time I was actively contributing to my GSoC organinzation. Not trying to showoff but I was the most active contributor. I made 44 PRs in 30 days. Out of which 6-7 PRs were new feature implementations. 19PRs were merged and other were under review. My state was like this, my final exams were going on but I was pushing my code changes 15 minutes ago before going to exam, to see the running CI test cases pass. So my gut was telling me that I would pass the GSoC but remember don\u0026rsquo;t ever be sure about anyting until you get the final result. Also one thing I want to say about GSoC selection is that:- Yes, contribution matter but what matter most is your proposal. So students who will be applying next year, remember this. It doesn\u0026rsquo;t matter if you made most contributions because most organizations mainly focus on your proposal.\nNow it was 1st April and OMP result were about to come out. I was not having any expectations and I was just walking down the road and listing to music when I got this email\n  omp selection   Serioulsy I was brain fucked at that time. I shouted Yaaas!!! and ran towards my room. My heart was pounding and didn\u0026rsquo;t matter. My situation was just like below.\nForrest Gump GIF from Ru GIFs I was running to check on Palash that did he got in too or not. Unfortunately he did not get it. Now when my excitment was over it was time to decide which one to select. OMP or GSoC(if I got selected). At that time GSoC result were not out but I was analyzing the case what I should do when I get it. Now my current case was like this. I had OMP and I was going to Atlan in Delhi to work in their office. Atlan was important because I did not had any office exeperience and I wanted it. OMP was important because I wanted to get an experience to work with mainframe and also it had great perks(I will mention later). Now it was May and GSoC results were out and I was selected. Now I was really confused what to do. Palash told me to do everything but he said I will be fucked mentally. To be honest at that time I also thought that too. My whole family said not to do all 3 and drop one of them.\nI was really confused but I wanted to do all 3 and I wanted to test myself. So I took all 3. Before going further what I want to say to all is, you might face similar situtaion in life and it totally depend on you what you want to do, but you should trust yourself and go forward with your gut. People will try to stop you but always trust your gut because it's you who will be facing the challenges not them. You should have faith that you can overcome them, beacuse when you do you will be a whole new person than before. You will be more confident, more agile and it will help you in your life.\nNow it\u0026rsquo;s time for my real experience. I was reallocated in Delhi with Palash. We were both working in Atlan. Let me be clear I won\u0026rsquo;t be criticing anyone I am just sharing my experience. So Atlan is a good company but the problem is that they don\u0026rsquo;t provide you accomodation and good stipend as other startups provide. So our first 3 days went by searching for accomodation. We used to walk like 25k everyday in search for a place to live. I wanted a place close to office because I could not waste any time in travelling. I had to manage 3 things at the same time. So after 3 days staying in OYO. We finally got a place to stay, with the help of a bhaiya in office. It was near from office but let me stop you right there. Let\u0026rsquo;s first look at the situation how we lived for 2 months in summer in Delhi.\n  living in delhi   Now let me describe our actual situation how we lived here. It was located on 4th floor(top) so heat was our best friend. There was no vantilation. If we opened fan, it was like just warming up to be killed. Now if we by mistake opened our cooler it was like we were saying please kill me now, don\u0026rsquo;t wait. Water that we got was just wow. I mean seriouly wow, like in summer who doesn\u0026rsquo;t want hot water. RIGHT??? We literally took steam bath for 2 months. Now let\u0026rsquo;s go to situation when we ate food. It was 50% of the time whenever we ordered our food at night the electricity went out. It was like always when we were eating food and our sweat was dropping in it. EWW, Right?? Well that was our situation. We were fortunate enough that Atlan provided food in office, so there was no problem with our breakfast and lunch.\nThis was my situation and I had to work on for GSoC and OMP. Now the best thing occured, I got fever. This time I thought it\u0026rsquo;s over let me quit from Atlan and go home. I was seriouly thinking about doing it. I did not had any strenght left at that time, I was disappointed with everything. Our living condition, work at office, my fever and I thought I couldn\u0026rsquo;t cope it. But my buddy Palash took care of me and I somehow got better from fever.\nThis was around end of May and my first evaluation of GSoC was about to come and I had not written much code. It was time I thought I should manage my time otherwise I will not be able to succeed. So I started to manage my time. Atlan has flexible working hours so we used to go to office around 11am and get back at 7pm. I used to take some break after office from 7-10pm and then focus on my GSoC work. I did most of my work at night and weekends. My OMP community bonding was also ended and coding period was also about to start. So there was another obstacle to handle. This was the reason we took only 2 months internship at Atlan from 13th May to 13th July. At that time my 2 evaluations of GSoC were about to be complete and OMP was going on.\nNow, thing that I want to say from above is that, first of all don\u0026rsquo;t give up. I know I was lucky to have a good friend by my side but you should also believe in yourself. Whenever you are thinking of quiting, always think of the reason why you started. This is only reason which kept me going. I wanted to prove myself that I can do it. I wanted to be confident in myself, so I pushed myself. Yes, I faced difficulties but I belived and overcame them. You should always be greatful for the opprotunities you are provided. Time management is one of the most important thing, if you want to do something. I learnt it the hard way.\nOur Delhi internship was over, and I was greatful I was going back to college. Now only last phase of GSoC and last phase of OMP was left. Time management was same because I had college to attend. In the end I did and completed both the projects.\nNow some of the perks I have facing because of OMP other than stipend is that, I will be presenting my project in Open Source Summit, Lyon, Europe. All expenses are paid.\nI am also greatful to both my mentors(OMP and GSoC) because they belived in me. Sometimes I lagged but I always completed my task before time. It was a good expereince.\nNow, I am going to Hackerrank this winter as a Software Developer Intern. So I will be in Banglore this winter. Thank you all for reading. Happy Coding.\nConclusion\nI wrote this to help others. Feel free to share this, so that other can also benefit from it.\n Follow me on Github to check what I am currently working on. My GSoC report OMP project and Bot repository.  Did you find this page helpful? Consider sharing it ðŸ™Œ ","date":1570838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608899017,"objectID":"a0d0b885203aa82da3dae15e7fe76f88","permalink":"/post/believe-in-yourself/","publishdate":"2019-10-12T00:00:00Z","relpermalink":"/post/believe-in-yourself/","section":"post","summary":"How I did 3 internships (OMP, GSoC and Atlan) at the same time.","tags":["GSoC","OMP","internship","opensource"],"title":"Believe in yourself","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["opensource","tech"],"content":"In the world of no one cares, I would like to tell everyone that being an open sourcer lover I would be contributing this year to Open Mainframe Project(OMP) and Google Summer of Code(GSoC). I am just telling this so that you might be interested in what I have to say in the next few paragraphs.\nSome few things I would like to say for the GSoC program. It is an awesome thing what google is doing to encourage young developers to work in the open source community. Students learn some real valuable skills while doing that. Whatever I will be saying next has nothing to do with the program, i.e I would not be criticizing about the program.\nThe BIG Misconception  I am awesome developer because I got selected to GSoC\n First of all, Big NOOOOO. No, no. I mean serious no. It\u0026rsquo;s awesome you got selected to GSoC but please don\u0026rsquo;t think that. There is an useless hype created by everyone that if anyone has done or has been selected for GSoC they are good developers. I am not saying that no one is not, but if you have applied for the program you might understand what I am trying to say. Let me try to explain by going a little bit deep.\nThere are more than 200 organizations selected every year for the program. What GSoC organizations try to see is that the student is interested in contributing to open source, they are active during GSoC selection period, they have basic coding skills and most important thing is that they have a good knowledge about the project they are applying. Now the main part where all goes south is, each org has different kind of projects every year according to their requirements like some have website building, working on an existing packages, creating a whole new software for the org etc and each project has it\u0026rsquo;s own level of difficulty. So the project you are working on might be easy than the others but they won\u0026rsquo;t admit the project is easy and they would go like \u0026ldquo;If it\u0026rsquo;s so easy can you do it\u0026rdquo;? Again I am not criticizing anyone, just putting the bitter truth out. A student\u0026rsquo;s selection depends from org to org. Every org has different selection criterion.\nNow the misconception is started by people because they hear the term Google. Let me be clear on that Google just sponsors the program, it has nothing to do with working for google. Another encouragment is made by some students who got seleted by showing off all over the place. If you have reached upto here you might have continue reading this because maybe you saw I was selected for GSoC(I said maybe). That\u0026rsquo;s what I am trying say. The program is exploited by Indian community and many good developers go not get through. One should also look at the student\u0026rsquo;s project quality rather than just the GSoC tag.\nFor those who did not get selected There are some awesome developers out there who applied but did not get selected. One of which is my best friend. Now, he did not get selected not because he did not work for GSoC period or because of proposal was bad(his mentor loved his proposal). But only because the org which he applied for(CNCF) did not give slots for the project. This is another important reason for which students do not get selected.\nSome motivation for you guys out there. Last year same thing happened to me but I forgot about it and next day I started working on some personal project and wrote this article. Not getting selected to GSoC is not the end, there are other awesome opportunities out there for you. Also I see most of students who only consider open source only for GSoC and if they are not selected they stop it. I would like to request, please don't do that. Open source is so awesome that it had taught me most of my developer skills. This is a community where you learn and help others learn. So don\u0026rsquo;t think of open source for GSoC period only. Even if you are not selected, continue contributing to other orgs and learn invaluable skills from the community.\nP.S:- Please mind my english and again I would like to emphasize on that I did not criticize any selected student or the program. Just putting out my views out there and some things people should know regarding the hype for GSoC.\nDid you find this page helpful? Consider sharing it ðŸ™Œ ","date":1557187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608897774,"objectID":"1ca3959d34d730fe52b81da70f62f179","permalink":"/post/hype-for-gsoc/","publishdate":"2019-05-07T00:00:00Z","relpermalink":"/post/hype-for-gsoc/","section":"post","summary":"Why Indian society has created so much hype for Google Summer of Code.","tags":["GSoC","opensource"],"title":"Hype for GSoC","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["opensource","tech"],"content":"This is Article 3 for this series. You can find first article here. and second article here.\nTable of Contents  About this Article Creating a Auth based system Accessing the API  Creating a Todo Updating a Todo Getting all Todo Deleting a Todo   Conclusion Code for this series Support  Did you find this page helpful? Consider sharing it ðŸ™Œ     About this Article In this article you will be able to do the following things:-\n  Create an Auth based system.\n  Use JWT to verify the tokens\n  CRUD(Create Read Update Delete) operations for Todo using GraphQL\n  Write tests for it.\n  Creating a Auth based system We need to have a system where users can create a todo only when they authenticate themselves as real user. For example, if we donâ€™t authenticate our routes, anyone can create a todo and spammers can spam the database. Also we need to keep track of todo for a particular user. Including them and many other reasons we need to create an Auth based system. So letâ€™s create a folder inside src called as middleware and then create 2 files inside it:-\n  auth.ts\n  index.ts\n  Inside src/middleware/auth.ts copy this :\nimport jwt = require(\u0026quot;jsonwebtoken\u0026quot;); import { model } from \u0026quot;mongoose\u0026quot;; import { completeRequest } from \u0026quot;../functions/complete\u0026quot;; import {Response} from \u0026quot;../models\u0026quot;; import {UserSchema} from \u0026quot;../schemas\u0026quot;; import {Config} from \u0026quot;../shared\u0026quot;; const User = model(\u0026quot;User\u0026quot;, UserSchema); /** * This is middleware to validate jwt token. * @param req * @param res * @param next */ export async function isAuthenticated(context) { const promise: Promise\u0026lt;Response\u0026gt; = new Promise((resolve, reject) =\u0026gt; { const token: any = context.headers[\u0026quot;x-access-token\u0026quot;]; const secret: any = Config.secretKeys.jwtSecret; if (!token) { reject(new Response(403, \u0026quot;Auth token missing\u0026quot;, { success: false, })); } else { // verify jwt token jwt.verify(token, secret, (err, decoded) =\u0026gt; { if (err) { reject(new Response(500, \u0026quot;Unable to authenticate user\u0026quot;, { success: false, })); } else { User.findById(decoded.id).select(\u0026quot;password\u0026quot;).then((user) =\u0026gt; { if (!user) { reject(new Response(400, \u0026quot;Sorry No user found\u0026quot;, { success: false, })); } else { resolve(new Response(200, \u0026quot;Successfull Response\u0026quot;, { success: true, user, token, })); } }); } }); } }); const val = await completeRequest(promise); return val; }  Explanation:-\n  Line 1â€“6:- We require necessary modules for this file.\n  Line 16:- We are exporting a function to check whether a user is authenticated or not. This functions takes an argument which context which is generally a form of request.\n  Line 17:- This function is returning a promise.\n  Line 18â€“23:- Now as said earlier context is in form of request so it will contain headers, now we will check if any token is passed in x-access-token header. This is a custom header that we will use to pass our JWT token when making a GraphQL request to CRUD operation on a Todo. If token is not present in header we reject our promise with a newresponse that we created in response.ts.\n  Line 26â€“30:- You can check here how a JWT works. So if any modification is made to our token then it will reject the with a new response.\n  Line 32â€“48:- If token is not tampered with then we recieve a decoded data. That decode data contains id of user. We then find that user with that id and return it. If no user is found we reject with a response.\n  Line 43â€“44:- We resolve the promise by passing it to completeRequest and gets back another promise but this promise has response in form we want. We then return the value.\n  After 2 articles you might have idea what we are going to write in src/middleware/index.ts. You may have already written it ðŸ˜„, but letâ€™s be sure and copy this code.\nexport * from \u0026quot;./auth\u0026quot;;  Now lets create our Schema for Todo for our mongoose model. Create a file inside schemas named todoSchema.ts and copy this inside\nsrc/schemas/todoSchema.ts\nimport { Schema } from \u0026quot;mongoose\u0026quot;; import mongoose = require(\u0026quot;mongoose\u0026quot;); mongoose.Promise = global.Promise; /** * This is Schema for Blog * @constant {BlogSchema} */ export const TodoSchema = new Schema({ id: { type: String, }, postedByid: { type: String, }, title: { type: String, required: true, select: true, }, description: { type: String, }, postedBy: { type: mongoose.Schema.Types.ObjectId, ref: \u0026quot;User\u0026quot;, }, name: { type: String, ref: \u0026quot;User\u0026quot;, }, }, { timestamps: {}, });  We already understood how to make a schema when we created userSchema. Here is just one new thing on line 25â€“28:- We need to reference the todo that is created to the user who created it. So postedBy will contain _id of user who will create that todo. Line 29â€“31 will store the name of the user.\nAlso update your src/schemas/index.ts and add\nexport * from \u0026quot;./todoSchema\u0026quot;;  Now we will create functions for CRUD operations on a todo. So create file todo.ts inside routes folder and copy this inside\nsrc/routes/todo.ts\nimport { model } from \u0026quot;mongoose\u0026quot;; import { completeRequest } from \u0026quot;../functions/complete\u0026quot;; import { Response } from \u0026quot;../models\u0026quot;; import { TodoSchema, UserSchema } from \u0026quot;../schemas\u0026quot;; const User = model(\u0026quot;User\u0026quot;, UserSchema); const Todo = model(\u0026quot;Todo\u0026quot;, TodoSchema); /** * This function creates a new Todo * @param args * @param user */ export async function addTodo(args, user) { const promise: Promise\u0026lt;Response\u0026gt; = new Promise((resolve, reject) =\u0026gt; { if (!args.input.title || !args.input.description) { reject(new Response(200, \u0026quot;Please enter both title and description\u0026quot;, { success: false, })); } const title = args.input.title.trim(); const description = args.input.description.trim(); if (!title.length || !description.length) { reject(new Response(200, \u0026quot;Title or description cannot be blank\u0026quot;, { success: false, })); } User.findById({ _id: user._id }, (err, user) =\u0026gt; { const todo = new Todo({ postedBy: user.id, name: user.name, title, description, }); todo.id = todo._id; todo.postedByid = user.id; todo.save((err) =\u0026gt; { if (err) { reject(new Response(200, \u0026quot;Error in saving Todo\u0026quot;, { success: false, })); } resolve(new Response(200, \u0026quot;Successfully saved Todo\u0026quot;, { success: true, todo, })); }); }); }); const val = await completeRequest(promise); return val; } /** * This returns all todos for user * @param user */ export async function getAlltodosForUser(user) { const promise: Promise\u0026lt;Response\u0026gt; = new Promise((resolve) =\u0026gt; { Todo.find({ postedBy: user._id }, (err, todos) =\u0026gt; { resolve(new Response(200, \u0026quot;All todos\u0026quot;, { success: true, todos, })); }); }); const val = await completeRequest(promise); return val; } /** * This updates the todo information * @param user */ export async function update(args, user) { const promise: Promise\u0026lt;Response\u0026gt; = new Promise((resolve, reject) =\u0026gt; { const todoId = args.input.id; if (!args.input.title || !args.input.description || !todoId) { reject(new Response(200, \u0026quot;Please enter all fields\u0026quot;, { success: false, })); } const title = args.input.title.trim(); const description = args.input.description.trim(); if (!title.length || !description.length) { reject(new Response(200, \u0026quot;Title or description cannot be blank\u0026quot;, { success: false, })); } Todo.findById({ _id: todoId }, (err, todo) =\u0026gt; { if (err) { reject(new Response(200, \u0026quot;Not able to get todo\u0026quot;, { success: false, })); } else if (String(todo.postedBy) !== String(user._id)) { reject(new Response(200, \u0026quot;You don't have access to update this todo\u0026quot;, { success: false, })); } else { Todo.findOneAndUpdate({ _id: todoId }, { $set: { title, description } }, { new: true }, (err, todo) =\u0026gt; { resolve(new Response(200, \u0026quot;Updated Todo\u0026quot;, { success: true, todo, })); }); } }); }); const val = await completeRequest(promise); return val; } /** * This function deletes the particular todo we want * @param args */ export async function deleteTodo(args, user) { const promise: Promise\u0026lt;Response\u0026gt; = new Promise((resolve, reject) =\u0026gt; { const todoId = args.id; Todo.findById({ _id: todoId }, (err, data) =\u0026gt; { if (err) { reject(new Response(200, \u0026quot;Not able to get todo\u0026quot;, { success: false, })); } else if (!data) { reject(new Response(200, \u0026quot;Todo already deleted\u0026quot;, { success: false, })); } else if (String(data.postedBy) !== String(user._id)) { reject(new Response(200, \u0026quot;You don't have access to delete this todo\u0026quot;, { success: false, })); } else { Todo.findOneAndDelete({ _id: todoId }, () =\u0026gt; { resolve(new Response(200, \u0026quot;Successfully deleted todo\u0026quot;, { success: true, })); }); } }); }); const val = await completeRequest(promise); return val; }  You might now have idea what we are doing. We have 4 functions.\n  addTodo :- This creates a new todo.\n  getAlltodosForUser :- This returns all todos for an authenticated user.\n  update :- This updates a particular Todo.\n  deleteTodo :- This deletes a particular todo.\n  All Functions are authenticated. That is, only authenticated user can make request to perform CRUD operation.\nNow lets create a graphQL schema for our Todo. Create a file todographqlSchema.ts inside schemas folder and copy this inside\nsrc/schemas/todographqlSchema.ts\nimport { GraphQLBoolean, GraphQLID, GraphQLInputObjectType, GraphQLInt, GraphQLList, GraphQLNonNull, GraphQLObjectType, GraphQLString } from \u0026quot;graphql\u0026quot;; // User type const todoType = new GraphQLObjectType({ name: \u0026quot;todo\u0026quot;, fields: { id: { type: GraphQLID }, postedByid: { type: GraphQLID }, name: { type: GraphQLString }, title: { type: GraphQLString }, description: { type: GraphQLString }, createdAt: { type: GraphQLString }, updatedAt: { type: GraphQLString }, }, }); // Data reponse of user const DataResponse = new GraphQLObjectType({ name: \u0026quot;todoDataResponse\u0026quot;, fields: { success: { type: GraphQLBoolean }, todo: { type: todoType }, token: { type: GraphQLString }, }, }); // Response from User const responseType = new GraphQLObjectType({ name: \u0026quot;toDoResponse\u0026quot;, fields: { code: { type: new GraphQLNonNull(GraphQLInt) }, message: { type: new GraphQLNonNull(GraphQLString) }, data: { type: new GraphQLNonNull(DataResponse) }, }, }); const toDoInput = new GraphQLInputObjectType({ name: \u0026quot;todoInput\u0026quot;, fields: { title: { type: GraphQLString }, description: { type: GraphQLString }, }, }); const toDoInputUpdate = new GraphQLInputObjectType({ name: \u0026quot;todoInputUpdate\u0026quot;, fields: { id: { type: GraphQLString }, title: { type: GraphQLString }, description: { type: GraphQLString }, }, }); // For getting todo // Data reponse of user const todoUsersDataResponse = new GraphQLObjectType({ name: \u0026quot;todoUsersDataResponse\u0026quot;, fields: { success: { type: GraphQLBoolean }, todos: { type: new GraphQLList(todoType) }, }, }); const userTodoResponse = new GraphQLObjectType({ name: \u0026quot;userTodoResponse\u0026quot;, fields: { code: { type: new GraphQLNonNull(GraphQLInt) }, message: { type: new GraphQLNonNull(GraphQLString) }, data: { type: new GraphQLNonNull(todoUsersDataResponse) }, }, }); // For deleting Todo const todoDeleteResponse = new GraphQLObjectType({ name: \u0026quot;todoDeleteResponse\u0026quot;, fields: { success: { type: GraphQLBoolean }, }, }); const userTodoDeleteResponse = new GraphQLObjectType({ name: \u0026quot;userTodoDeleteResponse\u0026quot;, fields: { code: { type: new GraphQLNonNull(GraphQLInt) }, message: { type: new GraphQLNonNull(GraphQLString) }, data: { type: new GraphQLNonNull(todoDeleteResponse) }, }, }); export const todographqlSchema = { todoType, DataResponse, responseType, toDoInput, toDoInputUpdate, todoUsersDataResponse, userTodoResponse, userTodoDeleteResponse, };  You might have idea about it, as it is somewhat similar to userLoginSchema.ts and userRegisterSchema.ts for graphQL. Also we need to export this file so paste this line inside\nsrc/schemas/index.ts\nexport * from \u0026quot;./todographqlSchema\u0026quot;;  Now we need to finally add queries and mutation inside our graphql.ts. Like we did for loginUser and registerUser. So copy and replace your old code with this inside\nsrc/schemas/graphql.ts\nimport { GraphQLNonNull, GraphQLObjectType, GraphQLSchema, GraphQLString } from \u0026quot;graphql\u0026quot;; import { isAuthenticated } from \u0026quot;../middleware\u0026quot;; import { addTodo, deleteTodo, getAlltodosForUser, login, register, update } from \u0026quot;../routes\u0026quot;; import { todographqlSchema } from \u0026quot;./todographqlSchema\u0026quot;; import { userLoginSchema } from \u0026quot;./userLoginSchema\u0026quot;; import { userRegisterSchema } from \u0026quot;./userRegisterSchema\u0026quot;; // Define the Query type const queryType = new GraphQLObjectType({ name: \u0026quot;Query\u0026quot;, fields: { loginUser: { type: new GraphQLNonNull(userRegisterSchema.responseType), // `args` describes the arguments that the `user` query accepts args: { input: { type: userLoginSchema.UserInput }, }, async resolve(_, args) { const val = await login(args); return val; }, }, profileUser: { type: new GraphQLNonNull(userRegisterSchema.responseType), // `args` describes the arguments that the `user` query accepts async resolve(parent, args, context, info) { const authenticated = await isAuthenticated(context); return authenticated; }, }, todoUsers: { type: new GraphQLNonNull(todographqlSchema.userTodoResponse), // `args` describes the arguments that the `user` query accepts async resolve(parent, args, context, info) { const authenticated = await isAuthenticated(context); if (authenticated.code !== 200) { return authenticated; } else { const val = await getAlltodosForUser(authenticated.data.user); return val; } }, }, }, }); // Defining Mutation const mutationType = new GraphQLObjectType({ name: \u0026quot;Mutation\u0026quot;, fields: { registerUser: { type: new GraphQLNonNull(userRegisterSchema.responseType), // `args` describes the arguments that the `user` query accepts args: { input: { type: userRegisterSchema.UserInput }, }, async resolve(_, args) { const val = await register(args); return val; }, }, addTodo: { type: new GraphQLNonNull(todographqlSchema.responseType), // `args` describes the arguments that the `user` query accepts args: { input: { type: todographqlSchema.toDoInput }, }, async resolve(parent, args, context, info) { const authenticated = await isAuthenticated(context); if (authenticated.code !== 200) { return authenticated; } else { const val = await addTodo(args, authenticated.data.user); return val; } }, }, updateTodo: { type: new GraphQLNonNull(todographqlSchema.responseType), // `args` describes the arguments that the `user` query accepts args: { input: { type: todographqlSchema.toDoInputUpdate }, }, async resolve(parent, args, context, info) { const authenticated = await isAuthenticated(context); if (authenticated.code !== 200) { return authenticated; } else { const val = await update(args, authenticated.data.user); return val; } }, }, deleteTodo: { type: new GraphQLNonNull(todographqlSchema.userTodoDeleteResponse), // `args` describes the arguments that the `user` query accepts args: { id: { type: GraphQLString }, }, async resolve(parent, args, context, info) { const authenticated = await isAuthenticated(context); if (authenticated.code !== 200) { return authenticated; } else { const val = await deleteTodo(args, authenticated.data.user); return val; } }, }, }, }); export const schema = new GraphQLSchema({ query: queryType, mutation: mutationType, });  Now lets write tests for our code changes. First we need to create 2 new files inside our test folder.\n  profileQueries.ts\n  todoQueries.ts\n  Inside src/test/profileQueries.ts copy this code\nconst query = `query profileUser{ profileUser { code message data { success token user { name } } } }`; const profileSuccessfullyQuery = { query: query, operationName: \u0026quot;profileUser\u0026quot; , }; export const profileUser = { profileSuccessfullyQuery, };  Also copy this to src/test/todoQueries.ts\nconst UserRoute = require(\u0026quot;./user-test.spec\u0026quot;); const todoId = UserRoute.toDoSavedData; const query = `mutation addTodo($input: todoInput) { addTodo(input: $input) { code message data { success todo { id postedByid description updatedAt createdAt name } } } }`; const updateQuery = `mutation updateTodo($input: todoInputUpdate) { updateTodo(input: $input) { code message data { success todo { id postedByid description updatedAt createdAt name title } } } }`; const deleteQuery = `mutation deleteTodo($id: String) { deleteTodo(id: $id) { code message data { success } } }`; const allTodos = `query todoUsers { todoUsers{ code message data { success todos { title description } } } }`; const toDoSuccessfullyQuery = { query: query, operationName: \u0026quot;addTodo\u0026quot; , variables: { input: { title: \u0026quot;Test title\u0026quot;, description: \u0026quot;test description\u0026quot;, }, }, }; const toDoFailNotitleOrDescyQuery = { query: query, operationName: \u0026quot;addTodo\u0026quot; , variables: { input: { title: \u0026quot;\u0026quot;, description: \u0026quot;\u0026quot;, }, }, }; const toDoFailNotitleQuery = { query: query, operationName: \u0026quot;addTodo\u0026quot; , variables: { input: { title: \u0026quot; \u0026quot;, description: \u0026quot;dasdasda\u0026quot;, }, }, }; const toDoUpdateQuery = { query: updateQuery, operationName: \u0026quot;updateTodo\u0026quot; , variables: { input: { id: \u0026quot;anything\u0026quot;, title: \u0026quot;Test title\u0026quot;, description: \u0026quot;test description\u0026quot;, }, }, }; const toDoFailNoIdUpdateQuery = { query: updateQuery, operationName: \u0026quot;updateTodo\u0026quot; , variables: { input: { id: \u0026quot;\u0026quot;, title: \u0026quot;fsdfs\u0026quot;, description: \u0026quot;fsdsdf\u0026quot;, }, }, }; const toDoFailNotitleOrDescUpdateQuery = { query: updateQuery, operationName: \u0026quot;updateTodo\u0026quot; , variables: { input: { id: \u0026quot;anything\u0026quot;, title: \u0026quot; \u0026quot;, description: \u0026quot; \u0026quot;, }, }, }; const toDoFailDeleteQuery = { query: deleteQuery, operationName: \u0026quot;deleteTodo\u0026quot; , variables: { id: \u0026quot;anything\u0026quot;, }, }; const TodoAllQuery = { query: allTodos, operationName: \u0026quot;todoUsers\u0026quot; , }; export const todoQueries = { toDoSuccessfullyQuery, toDoFailNotitleOrDescyQuery, toDoFailNotitleQuery, toDoUpdateQuery, toDoFailNoIdUpdateQuery, toDoFailNotitleOrDescUpdateQuery, toDoFailDeleteQuery, updateQuery, deleteQuery, TodoAllQuery, }; view raw  We need to update our user-test.spec.ts file. This file is very big, so open the link given below and copy the file into your src/test/user-test.spec.ts\nLets run our test by running npm run build \u0026amp;\u0026amp; npm run coverage. Make sure your MongoDB is up and running. You will get this output.\n Still ðŸ’¯ code coverage ðŸ˜‰\n Accessing the API First check your mongoDB server is up and running. Then start your server by running the following command\nnpm start\nNow to access the API of application open your GraphQL-Playground and enter url http://localhost:3000/graphql\nCreating a Todo Enter Query\nmutation addTodo($input: todoInput) { addTodo(input: $input) { code message data { success todo { id postedByid description updatedAt createdAt name } } } }  and then query variable\n{ \u0026quot;input\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;This is a test todo\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Lets see if this works\u0026quot; } }  Important:- You need to set HTTP HEADERS. You can get this token with the use of login API.\nquery loginUser($input: UserInputLogin) { loginUser(input: $input) { data { token } } }  and then query variable\n{ \u0026quot;input\u0026quot;: { \u0026quot;username\u0026quot;: \u0026quot;knrt10\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;test\u0026quot; } }  From this response copy the token and then copy this code to your HTTP HEADERS\n{ \u0026quot;x-access-token\u0026quot;: \u0026quot;your access token from login API\u0026quot; }  Then hit play button, you will get response like this. Try removing the token or changing token to something else and see the response.\ncreating a Todo\nUpdating a Todo Enter Query\nmutation updateTodo($input: todoInputUpdate) { updateTodo(input: $input) { code message data { success todo { id postedByid description updatedAt createdAt name title } } } }  and then query variable. You can get the id from the todo you created before.\n{ \u0026quot;input\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;id of your todo you created before\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Second check?\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Yaaho.\u0026quot; } }  You also need to set header like above. After that when you hit play button you will get this response\nupdating a Todo\nGetting all Todo Enter Query\nquery todoUsers { todoUsers{ code message data { success todos { title description id } } } }  and then set HTTP HEADERS like above API.\n{ \u0026quot;x-access-token\u0026quot;: \u0026quot;Your access token\u0026quot; }  After that when you hit play button you will get this response\nAll todos for users\nDeleting a Todo Enter Query\nmutation deleteTodo($id: String) { deleteTodo(id: $id) { code message data { success } } }  and then query variable. You can get the id from the todo you created before.\n{ \u0026quot;id\u0026quot;: \u0026quot;5c25156f70d37365ede03609\u0026quot; }  You also need to set header like above. After that when you hit play button you will get this response\ndeleting a Todo\nConclusion That is for this part. In this part you learnt following things:-\n  How to write Schema for GraphQL.\n  How to create Schema for Todo and link it with a User\n  Perform CRUD operation on a Todo using GraphQL.\n  How to write clean code.\n  How maintain ðŸ’¯ code coverage ðŸ˜‰.\n  You can see it took 33 tests to run in about 804ms. Which is less than a second. It shows how fast and precise our code is.\n  Code for this series Code is open sourced on Github under MIT license. Feel free to use it as reference if you are stuck anywhere.\nSupport I wrote this series of articles by using my free time. A little motivation and support helps me a lot. If you like this nifty hack you can support me by doing any (or all ðŸ˜‰ ) of the following:\n  Follow me on Github for more such projects.\n  â­ï¸ Star it on Github and make it trend so that other people can know about my project.\n  Did you find this page helpful? Consider sharing it ðŸ™Œ ","date":1546214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608897131,"objectID":"7b4dc285e5befa14bc94dc8400c0fdcf","permalink":"/post/part-3-api-using-graphql/","publishdate":"2018-12-31T00:00:00Z","relpermalink":"/post/part-3-api-using-graphql/","section":"post","summary":"This is a series of 3 articles which will help to write production grade code.","tags":["js","opensource","node.js","docker","mongodb","testing","grpahql"],"title":"Part 3:- API using GraphQL and Node.js","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["opensource","tech"],"content":"This is Article 2 for this series. You can find first article here. Our task now is to setup our graphQL API and work with it.\nTable of Contents  About this Article GraphQL is the better REST Data Fetching with REST vs GraphQL No more Over and Underfetching  Overfetching: Downloading superfluous data Underfetching and the n+1 problem   Benefits of a Schema \u0026amp; Type System Setting up GraphQL Accessing the API  Registering User to Database Login API   Docker Users Commiting our changes Conclusion Support  Did you find this page helpful? Consider sharing it ðŸ™Œ     About this Article In this article you will learn the following things:-\n  About GraphQL and why to use it\n  Setup GraphQL\n  Register User to Database\n  Login User and get token using JWT\n  Write tests for the above setup\n  GraphQL is the better REST Over the past decade, REST has become the standard (yet a fuzzy one) for designing web APIs. It offers some great ideas, such as stateless servers and structured access to resources. However, REST APIs have shown to be too inflexible to keep up with the rapidly changing requirements of the clients that access them.\nGraphQL was developed to cope with the need for more flexibility and efficiency! It solves many of the shortcomings and inefficiencies that developers experience when interacting with REST APIs.\nTo illustrate the major differences between REST and GraphQL when it comes to fetching data from an API, letâ€™s consider a simple example scenario: In a blogging application, an app needs to display the titles of the posts of a specific user. The same screen also displays the names of the last 3 followers of that user. How would that situation be solved with REST and GraphQL?\nData Fetching with REST vs GraphQL With a REST API, you would typically gather the data by accessing multiple endpoints. In the example, these could be /users/\u0026lt;id\u0026gt; endpoint to fetch the initial user data. Secondly, thereâ€™s likely to be a /users/\u0026lt;id\u0026gt;/posts endpoint that returns all the posts for a user. The third endpoint will then be the /users/\u0026lt;id\u0026gt;/followers that returns a list of followers per user.\n Image taken from other source\n In GraphQL on the other hand, youâ€™d simply send a single query to the GraphQL server that includes the concrete data requirements. The server then responds with a JSON object where these requirements are fulfilled.\n Image taken from other source\n No more Over and Underfetching One of the most common problems with REST is that of over- and underfetching. This happens because the only way for a client to download data is by hitting endpoints that return fixed data structures. Itâ€™s very difficult to design the API in a way that itâ€™s able to provide clients with their exact data needs.\n â€œThink in graphs, not endpoints.â€ Lessons From 4 Years of GraphQL by Lee Byron, GraphQL Co-Inventor.\n Overfetching: Downloading superfluous data Overfetching means that a client downloads more information than is actually required in the app. Imagine for example a screen that needs to display a list of users only with their names. In a REST API, this app would usually hit the /users endpoint and receive a JSON array with user data. This response however might contain more info about the users that are returned, e.g. their birthdays or addresses - information that is useless for the client because it only needs to display the usersâ€™ names.\nUnderfetching and the n+1 problem Another issue is underfetching and the n+1 requests problem. Underfetching generally means that a specific endpoint doesnâ€™t provide enough of the required information. The client will have to make additional requests to fetch everything it needs. This can escalate to a situation where a client needs to first download a list of elements, but then needs to make one additional request per element to fetch the required data.\nAs an example, consider the same app would also need to display the last three followers per user. The API provides the additional endpoint /users/\u0026lt;user-id\u0026gt;/followers. In order to be able to display the required information, the app will have to make one request to the /users endpoint and then hit the /users/\u0026lt;user-id\u0026gt;/followers endpoint for each user.\nBenefits of a Schema \u0026amp; Type System GraphQL uses a strong type system to define the capabilities of an API. All the types that are exposed in an API are written down in a schema using the GraphQL Schema Definition Language (SDL). This schema serves as the contract between the client and the server to define how a client can access the data.\nOnce the schema is defined, the teams working on frontend and backends can do their work without further communication since they both are aware of the definite structure of the data thatâ€™s sent over the network.\nFrontend teams can easily test their applications by mocking the required data structures. Once the server is ready, the switch can be flipped for the client apps to load the data from the actual API.\nSetting up GraphQL I hope I taught you little bit about graphQL, to know more you can check out here. GraphQL is a query language for your API, and a server-side runtime for executing queries by using a type system you define for your data. GraphQL isnâ€™t tied to any specific database or storage engine and is instead backed by your existing code and data. A GraphQL service is created by defining types and fields on those types, then providing functions for each field on each type. We will be using express-graphql for our application. So run this command in your terminal\nnpm i --quiet graphql express-graphql --save\nNon npm users just copy this to package.json\n\u0026quot;express-graphql\u0026quot;: \u0026quot;^0.7.1\u0026quot;, \u0026quot;graphql\u0026quot;: \u0026quot;^14.0.2\u0026quot;,  We will require our installed module and use this, so now copy this to your src/app.ts\nconst graphqlHTTP = require(\u0026quot;express-graphql\u0026quot;); import { schema } from \u0026quot;./schemas\u0026quot;;  Also change our member function private this.initAppRoutes() {} in src/app.ts to the code given below.\nprivate initAppRoutes() { this.app.use(\u0026quot;/graphql\u0026quot;, graphqlHTTP({ schema, graphiql: true, })); }  You will encounter some error, so lets work and solve them. Firstly, we need to create a schemas folder inside src. Then create a 5 files inside it\n  userSchema.ts\n  graphql.ts\n  userLoginSchema.ts\n  userRegisterSchema.ts\n  index.ts\n  Inside src/schemas/userSchema.ts copy this\nimport bcrypt = require(\u0026quot;bcrypt-nodejs\u0026quot;); import { Schema } from \u0026quot;mongoose\u0026quot;; import mongoose = require(\u0026quot;mongoose\u0026quot;); mongoose.Promise = global.Promise; /** * This is Schema for User * @constant {UserSchema} */ export const UserSchema = new Schema({ id: { type: String, }, username: { type: String, trim: true, unique: true, select: true, }, name: { type: String, select: true, required: true, }, password: { type: String, select: false, }, }, { timestamps: {}, }); UserSchema.methods.generateHash = function(password): boolean { return bcrypt.hashSync(password, bcrypt.genSaltSync(8), null); }; UserSchema.methods.validPassword = function(password): boolean { return bcrypt.compareSync(password, this.password); };  Explanation:-\n  This file defines mongoose schema for our database.\n  Line 1â€“3:- Requiring necessary module.\n  Line 11â€“32:- We are exporting our schema on line 11. On the same line we create a new Schema with properties specified below. Lets expain username property. username will be of type String and it will be unique. What select property does that is when we call this schema it will show the property of user which have select as true. You may notice we have specified select: false for password on line 28. It means we wonâ€™t be getting password by default for any queries we execute for user.\n  Line 34â€“40:- We defined 2 methods for users to generate a hashed password and validate that password.\n  Inside src/schemas/graphql.ts copy this\nimport { GraphQLNonNull, GraphQLObjectType, GraphQLSchema } from \u0026quot;graphql\u0026quot;; import { login, register } from \u0026quot;../routes\u0026quot;; import { userLoginSchema } from \u0026quot;./userLoginSchema\u0026quot;; import { userRegisterSchema } from \u0026quot;./userRegisterSchema\u0026quot;; // Define the Query type const queryType = new GraphQLObjectType({ name: \u0026quot;Query\u0026quot;, fields: { loginUser: { type: new GraphQLNonNull(userRegisterSchema.responseType), // `args` describes the arguments that the `user` query accepts args: { input: { type: userLoginSchema.UserInput }, }, async resolve(_, args) { const val = await login(args); return val; }, }, }, }); // Defining Mutation const mutationType = new GraphQLObjectType({ name: \u0026quot;Mutation\u0026quot;, fields: { registerUser: { type: new GraphQLNonNull(userRegisterSchema.responseType), // `args` describes the arguments that the `user` query accepts args: { input: { type: userRegisterSchema.UserInput }, }, async resolve(_, args) { const val = await register(args); return val; }, }, }, }); export const schema = new GraphQLSchema({ query: queryType, mutation: mutationType, });  Explanation:-\n  In this file we are defining our schema for graphQL here.\n  Line 7â€“22:- We define querytype as type query. In this we will be defining all our queries. fileds property tell all queries we can use. Line 10â€“15 will be like this for example.\n  query loginUser($input userInputLogin) { loginUser(input: $input){} where $input is { input { username: \u0026quot;anything\u0026quot;, password: \u0026quot;anything, }  . After this when query is written and input is provided line 16 will execute and it will asynchronously execute resolve function which is provided 2 arguments. args contains the input that we provided. After we wait for our login result as login returns a promise. We return the value on line 18. Similarly line 25â€“40 work but it is of type mutation, it is somewhat like POST request.\nInside src/schemas/userLoginSchema.ts copy this\nimport { GraphQLInputObjectType, GraphQLString } from \u0026quot;graphql\u0026quot;; // User input is getting input from user const UserInput = new GraphQLInputObjectType({ name: \u0026quot;UserInputLogin\u0026quot;, fields: { username: { type: GraphQLString }, password: { type: GraphQLString }, }, }); export const userLoginSchema = { UserInput, };  Explanation:-\n In this file we define the login Schema. It will be the return type of our input that we are providing.  Inside src/schemas/userRegisterSchema.ts copy this\nimport { GraphQLBoolean, GraphQLID, GraphQLInputObjectType, GraphQLInt, GraphQLNonNull, GraphQLObjectType, GraphQLString } from \u0026quot;graphql\u0026quot;; // User type const userType = new GraphQLObjectType({ name: \u0026quot;User\u0026quot;, fields: { id: { type: GraphQLID }, username: { type: GraphQLString }, name: { type: GraphQLString }, password: { type: GraphQLString }, token: { type: GraphQLString }, createdAt: { type: GraphQLString }, updatedAt: { type: GraphQLString }, }, }); // Data reponse of user const DataResponse = new GraphQLObjectType({ name: \u0026quot;DataResponse\u0026quot;, fields: { success: { type: GraphQLBoolean }, user: { type: userType }, token: { type: GraphQLString }, }, }); // Response from User const responseType = new GraphQLObjectType({ name: \u0026quot;Response\u0026quot;, fields: { code: { type: new GraphQLNonNull(GraphQLInt) }, message: { type: new GraphQLNonNull(GraphQLString) }, data: { type: new GraphQLNonNull(DataResponse) }, }, }); // User input is getting input from user const UserInput = new GraphQLInputObjectType({ name: \u0026quot;UserInputRegister\u0026quot;, fields: { username: { type: GraphQLString }, name: { type: GraphQLString }, password: { type: GraphQLString }, }, }); export const userRegisterSchema = { userType, DataResponse, responseType, UserInput, };  Explanation:-\n In this file we define the register Schema. It will be the return type of our input that we are providing.  And finally copy this to src/schemas/index.ts\nexport * from \u0026quot;./userSchema\u0026quot;; export * from \u0026quot;./graphql\u0026quot;; export * from \u0026quot;./userRegisterSchema\u0026quot;; export * from \u0026quot;./userLoginSchema\u0026quot;;  You will be getting error Cannot find module '../routes'. Donâ€™t worry we will fix it later. Letâ€™s first create some important things. Create a folder models and 2 files inside it\n  response.ts\n  index.ts\n  Copy this inside src/models/response.ts\nexport class Response { public code: number; public message: string; public data: any; constructor(code: number, message: string, data: any) { this.code = code; this.message = message; this.data = data; } }  Copy this inside src/models/index.ts :\nexport * from \u0026quot;./response\u0026quot;;  To modulerize our code we will create another folder functions and a file inside it\n complete.ts  Copy this inside src/functions/complete.ts :\nimport { Response } from \u0026quot;../models\u0026quot;; export function completeRequest(promise: Promise\u0026lt;Response\u0026gt;): any { const res = promise.then((response) =\u0026gt; { const finallResponse = { code: response.code, message: response.message, data: response.data, }; return finallResponse; }).catch((errorRes) =\u0026gt; { const finallResponse = { code: errorRes.code, message: errorRes.message, data: errorRes.data, }; return finallResponse; }); return res; }  This file returns a promise that contains final response of data. We resolve this promise in our graphql.ts file.\nBefore solving our routes folder issue we will first create a folder called interface and then create 2 files inside it, given below:-\n  userInterface.ts\n  index.ts\n  One of TypeScriptâ€™s core principles is that type-checking focuses on the shape that values have. This is sometimes called â€œduck typingâ€ or â€œstructural subtypingâ€. In TypeScript, interfaces fill the role of naming these types, and are a powerful way of defining contracts within your code as well as contracts with code outside of your project.\ncopy this inside src/interfaces/userInteface.ts :\nimport {Document} from \u0026quot;mongoose\u0026quot;; /** * This is interface for user * @interface * @extends {Document} */ export interface IUser extends Document { // tslint:disable-next-line:semicolon username: string, // tslint:disable-next-line:semicolon password: string, // tslint:disable-next-line:semicolon _id: any, // tslint:disable-next-line:semicolon name: string, }  And copy this inside src/interfaces/index.ts\nexport * from \u0026quot;./userInterface\u0026quot;;  Now everthing is complete. Its time to create routes folder. Create 2 files inside it\n  user.ts\n  index.ts\n  And copy this inside src/routes/user.ts\nimport jwt = require(\u0026quot;jsonwebtoken\u0026quot;); import { model } from \u0026quot;mongoose\u0026quot;; import { completeRequest } from \u0026quot;../functions/complete\u0026quot;; import { IUser } from \u0026quot;../interfaces\u0026quot;; import { Response } from \u0026quot;../models\u0026quot;; import { UserSchema } from \u0026quot;../schemas\u0026quot;; import { Config } from \u0026quot;../shared\u0026quot;; const User = model(\u0026quot;User\u0026quot;, UserSchema); /** * This is route for registering user in database * @param args */ export async function register(args) { const promise: Promise\u0026lt;Response\u0026gt; = new Promise\u0026lt;Response\u0026gt;((resolve, reject) =\u0026gt; { const secret: any = Config.secretKeys.jwtSecret; // getting data from args const username = String(args.input.username).trim(); if (!username || !args.input.username || !args.input.name) { reject(new Response(200, \u0026quot;Please fill both username and name\u0026quot;, { success: false, })); } else if (username.length \u0026lt; 4 || args.input.name.trim().length \u0026lt; 4) { reject(new Response(200, \u0026quot;Username and name should be contain atleast 4 characters\u0026quot;, { success: false, })); } else { const name = args.input.name.trim(); User.findOne({ username }).then((user: any) =\u0026gt; { if (user !== null) { reject(new Response(200, \u0026quot;username already in use\u0026quot;, { success: false, })); } else { const newUser: any = new User({ username, name, }); newUser.id = newUser._id; // generating new hashed password newUser.password = newUser.generateHash(args.input.password); newUser.save().then((user: IUser) =\u0026gt; { const token = jwt.sign({ id: user._id }, secret, { expiresIn: \u0026quot;23h\u0026quot;, }); resolve(new Response(200, \u0026quot;Successful response\u0026quot;, { success: true, user, token, })); }); } }); } }); const val = await completeRequest(promise); return val; } export async function login(args) { const promise: Promise\u0026lt;Response\u0026gt; = new Promise\u0026lt;Response\u0026gt;((resolve, reject) =\u0026gt; { if (!args.input.username || !args.input.password) { reject(new Response(200, \u0026quot;Please enter both field username and password\u0026quot;, { success: false, })); } // Getting data from req.body const username = args.input.username; const secret: any = Config.secretKeys.jwtSecret; // Searching for User in database User.findOne({ username }).select(\u0026quot;password id createdAt updatedAt\u0026quot;).then((user: any) =\u0026gt; { if (!user) { reject(new Response(200, \u0026quot;Sorry, No user found\u0026quot;, { success: false, })); } else { if (!user.validPassword(args.input.password)) { reject(new Response(200, \u0026quot;Incorrect Password\u0026quot;, { success: false, })); } else { const token = jwt.sign({ id: user._id }, secret, { expiresIn: \u0026quot;23h\u0026quot;, }); resolve(new Response(200, \u0026quot;Successful response\u0026quot;, { success: true, user, token, })); } } }); }); const val = await completeRequest(promise); return val; }  Explanation:-\n  Line 1â€“7:- We require necessary modules.\n  Line 9:- We create a model for our userSchema.\n  Line 16:- We export our function to register user to database. This function is returning a promise. The parameter args contains input from user.\n  Line 22â€“29:- We write tests for wrong input from user and reject our promise with our Response class that we created in response.ts.\n  Line 32â€“37:- If everything from user side looks fine, we check whether the username entered by user is already in database or not. If yes we reject with another new response.\n  Line 38â€“41:- We create a new User.\n  Lin 45:- We hash the password using the method we created in userSchema.ts.\n  Line 52â€“57:- If everything is fine, we resolve our promise and send the required info.\n  Line 62â€“63:- Our function completeRequest defined in functions folder, takes a promise and resolves it and send back another promise with some our required data. Then we use use await to wait for our data, until it is returned to us and stored inside val. Then we return val.\n  Similary like this we are working with login function starting from line 66.\n  Before running our code, lets create our tests for the code we have written so far. Create 2 files inside src/test\n  registerUserQueries.ts\n  loginUserQueries.ts\n  Copy this inside src/test/registerUserQueries.ts\nconst query = `mutation registerUser($input: UserInputRegister) { registerUser(input: $input) { code message data { token success user { id createdAt username name password updatedAt } } } } `; const registerSuccessfullyQuery = { query: query, operationName: \u0026quot;registerUser\u0026quot; , variables: { input: { username: \u0026quot;knrt10\u0026quot;, name: \u0026quot;Kautilya\u0026quot;, password: \u0026quot;test\u0026quot;, }, }, }; const registerSuccessfullyQuerySecondUser = { query: query, operationName: \u0026quot;registerUser\u0026quot; , variables: { input: { username: \u0026quot;knrt191\u0026quot;, name: \u0026quot;Second\u0026quot;, password: \u0026quot;test\u0026quot;, }, }, }; const registerSuccessfullyQueryThirdUser = { query: query, operationName: \u0026quot;registerUser\u0026quot; , variables: { input: { username: \u0026quot;knrt1912\u0026quot;, name: \u0026quot;Third\u0026quot;, password: \u0026quot;test\u0026quot;, }, }, }; const registerFailNoUsernameQuery = { query: query , operationName: \u0026quot;registerUser\u0026quot; , variables: { input: { username: \u0026quot;\u0026quot;, name: \u0026quot;Kautilya\u0026quot;, password: \u0026quot;test\u0026quot;, }, }, }; const registerFailSmallUsernameQuery = { query: query, operationName: \u0026quot;registerUser\u0026quot; , variables: { input: { username: \u0026quot;d \u0026quot;, name: \u0026quot;Kautilya\u0026quot;, password: \u0026quot;test\u0026quot;, }, }, }; export const registerqueries = { registerSuccessfullyQuery, registerSuccessfullyQuerySecondUser, registerSuccessfullyQueryThirdUser, registerFailNoUsernameQuery, registerFailSmallUsernameQuery, };  Explanation:- In this file we are creating a demo possible queries for register API which user can pass. We use this in our tests.\n  Line 1â€“19:- It contains our query that we will use later to register user, but in our tests we need our code to automatically create user, so we use this method.\n  Line 21â€“32:- We are defining a constant that is just an example of data we will be sending to our test route. It is using query that we defined on line 1. The operation name will be registerUser. Also we are passing variables to our query declared on line 25.\n  Rest file contains same type of different cases that a user can enter. We will test those all cases in our tests.\n  In the end we are exporting all the constants that we declared in this file, so that they can be used in other files.\n  And copy this inside src/test/loginUserQueries.ts\nconst query = `query loginUser($input: UserInputLogin) { loginUser(input: $input) { code, message, data { success user { id name username password } token } } }`; const loginSuccessfullyQuery = { query: query, operationName: \u0026quot;loginUser\u0026quot; , variables: { input: { username: \u0026quot;knrt10\u0026quot;, password: \u0026quot;test\u0026quot;, }, }, }; const loginFailWrongPasswordQuery = { query: query, operationName: \u0026quot;loginUser\u0026quot; , variables: { input: { username: \u0026quot;knrt10\u0026quot;, password: \u0026quot;test1\u0026quot;, }, }, }; const loginFailNopassWordorUsernameQuery = { query: query, operationName: \u0026quot;loginUser\u0026quot; , variables: { input: { username: \u0026quot;\u0026quot;, password: \u0026quot;\u0026quot;, }, }, }; const loginFailwrongUsernamQuery = { query: query, operationName: \u0026quot;loginUser\u0026quot; , variables: { input: { username: \u0026quot; bla \u0026quot;, password: \u0026quot;shit this is bro\u0026quot;, }, }, }; export const loginQueries = { loginSuccessfullyQuery, loginFailWrongPasswordQuery, loginFailNopassWordorUsernameQuery, loginFailwrongUsernamQuery, };  Explanation:- In this file we are creating a demo possible queries for login API which user can pass. We use this in our tests.\n  Just like registerQueries, this file has a query declared on line 1, which we will use in our test.\n  Cases that user can enter are declared in rest of file and then finally exported to be used later on in our test file.\n  Now you need to make changes to your src/test/user-test.spec.ts Update your whole file to this.\nimport chai = require(\u0026quot;chai\u0026quot;); import chaiAsPromised = require(\u0026quot;chai-as-promised\u0026quot;); import chaiHttp = require(\u0026quot;chai-http\u0026quot;); import { suite, test } from \u0026quot;mocha-typescript\u0026quot;; import { model } from \u0026quot;mongoose\u0026quot;; import sinon = require(\u0026quot;sinon\u0026quot;); import { Response } from \u0026quot;../models\u0026quot;; import { UserSchema } from \u0026quot;../schemas\u0026quot;; import { TodoApp } from \u0026quot;../server\u0026quot;; import { Config } from \u0026quot;../shared\u0026quot;; import { loginQueries } from \u0026quot;./loginUserQueries\u0026quot;; import { registerqueries } from \u0026quot;./registerUserQueries\u0026quot;; const User = model(\u0026quot;User\u0026quot;, UserSchema); // starting the server const server: TodoApp = new TodoApp(process.env.API_PORT || 3001); server.startServer(); chai.use(chaiAsPromised); chai.use(chaiHttp); @suite(\u0026quot;User Test class\u0026quot;) class UserTests { static user: any; static before() { this.testData = { input: { username: \u0026quot;knrt10\u0026quot;, name: \u0026quot;Kautilya\u0026quot;, password: \u0026quot;test\u0026quot;, }, }; } static after() { // Delete User Created So that it does not provide error in next test User.findOneAndDelete({ username: UserTests.testData.input.username }, () =\u0026gt; { process.exit(0); }); } private static testData: any; private static token: string; @test(\u0026quot;Testing Local Connection - try connection for Local mongodb\u0026quot;) public localDb(done) { setTimeout(() =\u0026gt; { Config.dbSettings.localDatabase = true; const mock = sinon.mock(new TodoApp(process.env.API_PORT || 3001), \u0026quot;constructor\u0026quot;); chai.expect(mock.object.infoString).to.deep.equal(\u0026quot;mongodb://\u0026quot; + Config.dbSettings.connectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database); done(); }, 100); } @test(\u0026quot;Testing Docker Connection - try connection for docker mongodb\u0026quot;) public dockerDb(done) { Config.dbSettings.localDatabase = false; const mock = sinon.mock(new TodoApp(process.env.API_PORT || 3001), \u0026quot;constructor\u0026quot;); chai.expect(mock.object.infoString).to.deep.equal(\u0026quot;mongodb://\u0026quot; + Config.dbSettings.dockerconnectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database); done(); } @test(\u0026quot;Testing Online Connection - try connection for online mongodb\u0026quot;) public OnlineDb(done) { Config.dbSettings.authEnabled = true; const mock = sinon.mock(new TodoApp(process.env.API_PORT || 3001), \u0026quot;constructor\u0026quot;); chai.expect(mock.object.infoString).to.deep.equal(\u0026quot;mongodb://\u0026quot; + Config.dbSettings.username + \u0026quot;:\u0026quot; + Config.dbSettings.password + \u0026quot;@\u0026quot; + Config.dbSettings.connectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database); done(); } @test(\u0026quot;POST Register - try Register User Successfuly\u0026quot;) public createUser(done) { chai.request(\u0026quot;http://localhost:\u0026quot; + server.port) .post(\u0026quot;/graphql\u0026quot;) .send(registerqueries.registerSuccessfullyQuery) .end((err, res) =\u0026gt; { chai.expect(res).to.have.status(200); chai.expect(res.body.data.registerUser).to.deep.equal(new Response(200, \u0026quot;Successful response\u0026quot;, { success: true, user: res.body.data.registerUser.data.user, token: res.body.data.registerUser.data.token, })); done(); }); } @test(\u0026quot;POST Register - Don't register as user already registered\u0026quot;) public dontRegisterUser(done) { chai.request(\u0026quot;http://localhost:\u0026quot; + server.port) .post(\u0026quot;/graphql\u0026quot;) .send(registerqueries.registerSuccessfullyQuery) .end((err, res) =\u0026gt; { chai.expect(res).to.have.status(200); chai.expect(res.body.data.registerUser).to.deep.equal(new Response(200, \u0026quot;username already in use\u0026quot;, { success: false, token: null, user: null, })); done(); }); } @test(\u0026quot;POST Register - try No username field\u0026quot;) public dontCreateUser(done) { chai.request(\u0026quot;http://localhost:\u0026quot; + server.port) .post(\u0026quot;/graphql\u0026quot;) .send(registerqueries.registerFailNoUsernameQuery) .end((err, res) =\u0026gt; { chai.expect(res).to.have.status(200); chai.expect(res.body.data.registerUser).to.deep.equal(new Response(200, \u0026quot;Please fill both username and name\u0026quot;, { success: false, token: null, user: null, })); done(); }); } @test(\u0026quot;POST Register - try username of small length\u0026quot;) public dontCreateUserLessLength(done) { chai.request(\u0026quot;http://localhost:\u0026quot; + server.port) .post(\u0026quot;/graphql\u0026quot;) .send(registerqueries.registerFailSmallUsernameQuery) .end((err, res) =\u0026gt; { chai.expect(res).to.have.status(200); chai.expect(res.body.data.registerUser).to.deep.equal(new Response(200, \u0026quot;Username and name should be contain atleast 4 characters\u0026quot;, { success: false, token: null, user: null, })); done(); }); } @test(\u0026quot;POST Login - try Successful Login\u0026quot;) public login(done) { chai.request(\u0026quot;http://localhost:\u0026quot; + server.port) .post(\u0026quot;/graphql\u0026quot;) .send(loginQueries.loginSuccessfullyQuery) .end((err, res) =\u0026gt; { UserTests.user = res.body.data.loginUser.data.user; UserTests.token = res.body.data.loginUser.data.token; chai.expect(res).to.have.status(200); chai.expect(res.body.data.loginUser).to.deep.equal(new Response(200, \u0026quot;Successful response\u0026quot;, { success: true, user: res.body.data.loginUser.data.user, token: res.body.data.loginUser.data.token, })); done(); }); } @test(\u0026quot;POST Login - try hit the login with incorrect credentials route\u0026quot;) public loginWithIncorrect(done) { chai.request(\u0026quot;http://localhost:\u0026quot; + server.port) .post(\u0026quot;/graphql\u0026quot;) .send(loginQueries.loginFailWrongPasswordQuery) .end((err, res) =\u0026gt; { chai.expect(res).to.have.status(200); chai.expect(res.body.data.loginUser).to.deep.equal(new Response(200, \u0026quot;Incorrect Password\u0026quot;, { success: false, user: null, token: null, })); done(); }); } @test(\u0026quot;POST Login - try hit the login no password\u0026quot;) public wrongInputFields(done) { chai.request(\u0026quot;http://localhost:\u0026quot; + server.port) .post(\u0026quot;/graphql\u0026quot;) .send(loginQueries.loginFailNopassWordorUsernameQuery) .end((err, res) =\u0026gt; { chai.expect(res).to.have.status(200); chai.expect(res.body.data.loginUser).to.deep.equal(new Response(200, \u0026quot;Please enter both field username and password\u0026quot;, { success: false, user: null, token: null, })); done(); }); } @test(\u0026quot;POST Login - try Posting wrong username\u0026quot;) public NoUser(done) { chai.request(\u0026quot;http://localhost:\u0026quot; + server.port) .post(\u0026quot;/graphql\u0026quot;) .send(loginQueries.loginFailwrongUsernamQuery) .end((err, res) =\u0026gt; { chai.expect(res).to.have.status(200); chai.expect(res.body.data.loginUser).to.deep.equal(new Response(200, \u0026quot;Sorry, No user found\u0026quot;, { success: false, user: null, token: null, })); done(); }); } }  Explanation:-\n  Line 1â€“12:- We require all the necessary files and modules.\n  Line 14:- We create a user model for our tests.\n  Line 16â€“17:- We start the server of our express application.\n  Line 26â€“34:- before is a method on chai that executes before our tests start running. So anything declared inside it will execute first.\n  Line 36â€“41:- Similarly like before method, after method is a predefined method in chai. It will be executed after all the tests have run. It does not depend your tests pass or fail. In Our tests are running inside a class, so we need to declare these methods as static which means that it doesnâ€™t have to be instantiated. In this method we are deleting user that we will create in our tests, so that next that we run our test that user is not already present in our database.\n  Line 73â€“87:- This tests is checking whether user gets successfully registered to database. We create a post request with data we created in our registerUserQueries.ts. And then this code send data to our code written in user.ts and if returns a response. Then we check assertion wheter the response matches with the required response in our test.\n  Just like this we create different test cases for all possible situation. Just read the message inside @test(\u0026quot;\u0026quot;), you will understand what test case it is about.\nRemember before running tests your mongoDb server is up and running then when you run npm run build \u0026amp;\u0026amp; npm run coverage you get\n Project setup and tests until now with ðŸ’¯ code coverage\n Accessing the API First check your mongoDB server is up and running. Then start your server by running the following command\nnpm start\nTo access the API of application open your GraphQL-Playground and enter url http://localhost:3000/graphql\nRegistering User to Database Enter Query\nmutation registerUser($input: UserInputRegister) { registerUser(input: $input) { code message data { token success user { id createdAt username name password updatedAt } } } }  and then query variable\n{ \u0026quot;input\u0026quot;: { \u0026quot;username\u0026quot;: \u0026quot;knrt10\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Kautilya\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;test\u0026quot; } }  Then hit play button, you will get response like this\n registering user to Database\n Login API Enter Query\nquery loginUser($input: UserInputLogin) { loginUser(input: $input) { code, message, data { success user { id name username password } token } } }  and then query variable\n{ \u0026quot;input\u0026quot;: { \u0026quot;username\u0026quot;: \u0026quot;knrt10\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;test\u0026quot; } }  Then after you hit play button you will see this output.\n login response\n Try playing with it and try to enter wrong data or anything like that. You might find we have covered error for most of the usual cases.\n playing with GraphQL Playground ðŸ“¹\n Docker Users You need to make all the code changes as above and then just run\nnpm run dockerStart\nAfter that you can also excess API of application as above with same URL http://localhost:3000/graphql.\nThen after using donâ€™t forget to stop the container by runnnig npm run dockerStop.\nCommiting our changes Lets commit our new changes for this part. Copy and run the commands given below inside your git repository. Make sure you are in root folder.\ngit add . git commit -m \u0026quot;Adding part2 changes\u0026quot;  Conclusion That is for this part. In this part you learnt following things:-\n  Modularize code.\n  How to create Mongoose Schema.\n  Why GraphQL is better than REST.\n  How to setup GraphQL in node.\n  How to write Schema for GraphQL.\n  How and write tests and maintain ðŸ’¯ code coverage ðŸ˜‰.\n  In next part you will create schemas for working with CRUD operation on a Todo, only when you are authenticated.\n  Support I wrote this series of articles by using my free time. A little motivation and support helps me a lot. If you like this nifty hack you can support me by doing any (or all ðŸ˜‰ ) of the following:\n  Follow me on Github for more such projects.\n  â­ï¸ Star it on Github and make it trend so that other people can know about my project.\n  Did you find this page helpful? Consider sharing it ðŸ™Œ ","date":1546128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608897131,"objectID":"42fbe150cc9a38ff0fe07171a0ba29d3","permalink":"/post/part-2-api-using-graphql/","publishdate":"2018-12-30T00:00:00Z","relpermalink":"/post/part-2-api-using-graphql/","section":"post","summary":"This is a series of 3 articles which will help to write production grade code.","tags":["js","opensource","node.js","docker","mongodb","testing","grpahql"],"title":"Part 2:- API using GraphQL and Node.js","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["opensource","tech"],"content":"This article is for people who are interested in learning to write optimized code using GraphQL with unit testing using mocha and chai.\nBefore we start, we need to define the functionality of our API. The application will be a simple todo app. It will create a user in a database who will be able to create, get and delete todos. If you donâ€™t understand the code or get stuck somewhere, you can check out the code from my Github repository.\nTable of Contents  About the series What you will learn? In this Article Requirements Project Setup Docker Setup  Exploring container from within   Setting up Express server Commiting our changes Conclusion  Did you find this page helpful? Consider sharing it ðŸ™Œ     About the series This is a series of 3 articles which will help to write production grade code. I wanted to help others who might have faced same problems as me. This article can help a beginner and also to someone who has good knowledge in node.\nWhat you will learn?  TypeScript (if you donâ€™t write it already). Using Docker for Node projects. Writing a GraphQL API instead of REST. Basics of MongoDB. Logging. Working with JWT(Json Web token). Writing tests and working with code coverage. Writing clean code and linting. Writing modular code. Basics of Promises, async and await. Writing production grade code.  In this Article In this article you will be able to do the following things:-\n Setup project Setup docker Write express server and connect to mongodb Write tests for above setup.  Requirements   Node and npm installed for your OS. We will use node environment to work on this project.\n  Docker installed for your OS. Along with local working of project, you will learn little bit of docker and how to work on projects if you donâ€™t have node and npm installed.\n  GraphQL-Playground for testing your API. This is an awesome tool where you will test your APIs and learn how to use this tool.\n  Important :- I will teach how to work with project locally and also how to work with Docker side by side. Those who want to work only with docker, I will refer to them as non npm users. Also we will try to follow TDD(test driven Development) approach.\n  Project Setup Open your development folder and create a new Folder for our project and go to it or copy the following command.\nmkdir graphql-todo\nNow open your code editor inside that folder.\nIf you have npm installed for your OS run\nnpm init -y. It will create a package.json file like this\n{ \u0026quot;name\u0026quot;: \u0026quot;graphql-todo\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;main\u0026quot;: \u0026quot;index.js\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;test\u0026quot;: \u0026quot;echo \\\u0026quot;Error: no test specified\\\u0026quot; \u0026amp;\u0026amp; exit 1\u0026quot; }, \u0026quot;keywords\u0026quot;: [], \u0026quot;author\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;ISC\u0026quot; }  non npm users just create a package.json file and copy the above contents. You can edit description to anything you like.\nWe will be writing code in Typescript so we will need something that will compile our code fast and minify the process. So for this we will use Gulp. Run the given command below in your terminal inside the project.\nnpm i --quiet gulp gulp-sourcemaps gulp-typescript typescript --save-dev non npm users copy this to your package.json file.\n\u0026quot;devDependencies\u0026quot;: { \u0026quot;gulp\u0026quot;: \u0026quot;^4.0.0\u0026quot;, \u0026quot;gulp-sourcemaps\u0026quot;: \u0026quot;^2.6.4\u0026quot;, \u0026quot;gulp-typescript\u0026quot;: \u0026quot;^5.0.0\u0026quot;, \u0026quot;typescript\u0026quot;: \u0026quot;^3.2.2\u0026quot; }  Now you have gulp and typescript installed. We will now create a gulpfile and write our code to minify the process of building the project. Follow the step below.\nCreate a gulpfile.js inside root directory and copy this code.\nconst gulp = require(\u0026quot;gulp\u0026quot;); const ts = require(\u0026quot;gulp-typescript\u0026quot;); const sourcemaps = require(\u0026quot;gulp-sourcemaps\u0026quot;); const tsProject = ts.createProject(\u0026quot;tsconfig.json\u0026quot;, { typescript: require(\u0026quot;typescript\u0026quot;) }); gulp.task(\u0026quot;build\u0026quot;, () =\u0026gt; { gulp.src(\u0026quot;process.yml\u0026quot;) .pipe(gulp.dest(\u0026quot;dist\u0026quot;)); return tsProject.src() .pipe(sourcemaps.init()) .pipe(tsProject()) .js .pipe(sourcemaps.write()) .pipe(gulp.dest(\u0026quot;dist\u0026quot;)); }); gulp.task(\u0026quot;watchTask\u0026quot;, function () { gulp.watch(\u0026quot;src/**/*.ts\u0026quot;, [\u0026quot;build\u0026quot;]); }); gulp.task(\u0026quot;default\u0026quot;, gulp.series(\u0026quot;build\u0026quot;)); gulp.task(\u0026quot;watch\u0026quot;, gulp.series(\u0026quot;build\u0026quot;, \u0026quot;watchTask\u0026quot;));  Explanation\n  Line 1â€“3:- We are requiring our modules.\n  Line 4: We are using API of gulp-typescript and creating our TS project using tsconfig.json file which we will create later.\n  Line 8:- We are using Gulp API task and creating a task name build.\n  Line 9â€“10:- We are using Gulp API src and taking process.yml file as source(we will create it later) and piping it to our destination folder **dist.**Here gulp.src creates a readble stream and with the help of node streams we pipe it to a writable stream created by gulp.dest.\n  Line 12â€“17:- We are copying our whole code as per tsconfig.json and converting it to Javascript and pasting it to dist folder.\n  Line 20â€“22:- We are creating another task watchTask for watching any changes in our build task.\n  Line 24â€“25:- We are finally using Gulpâ€™s default API for runnig build task and watch API for watching changes to build and watch task.\n  Create a tsconfig.json file in root directory and copy this code.\n{ \u0026quot;compilerOptions\u0026quot;: { \u0026quot;emitDecoratorMetadata\u0026quot;: true, \u0026quot;experimentalDecorators\u0026quot;: true, \u0026quot;outDir\u0026quot;: \u0026quot;./dist\u0026quot;, \u0026quot;moduleResolution\u0026quot;: \u0026quot;node\u0026quot;, \u0026quot;baseUrl\u0026quot;: \u0026quot;./src\u0026quot;, \u0026quot;sourceMap\u0026quot;: true, \u0026quot;pretty\u0026quot;: true, \u0026quot;strictNullChecks\u0026quot;: true, \u0026quot;module\u0026quot;: \u0026quot;commonjs\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;es6\u0026quot;, \u0026quot;allowJs\u0026quot;: true }, \u0026quot;include\u0026quot;: [ \u0026quot;src/**/*.ts\u0026quot; ], \u0026quot;exclude\u0026quot;: [ \u0026quot;node_modules\u0026quot; ] }  Important:- Do notice baseUrl and outDir. baseUrl is where we will write our TS code and outDir is where gulp will compile and copy our code to JavaScript.\nNow create a process.yml file in root directory and copy this code.\napps: - script : 'app.js' name : 'Backend To-do Application' node_args : '--inspect=0.0.0.0:5858'  We will learn why we are creating this file later on. After this create an empty directory src in our root folder. Up until now you might have project config like this. Those who are going to work only with docker, they will not have package-lock.json and node_modules.\nProject setup until now\nTo keep track of our work we will be using git. If not installed you can go through git to install it.\nIn your project initialize your repository as git repository by git init.\nFor working in developement version we will need nodemon and ts-node for reloading our server automatically when any change is made in our file and executing our TS file without building them respectively. We will use TSlint as our code linter. In your terminal inside project directory run this command.\nnpm i --quiet nodemon ts-node tslint tslint-eslint-rules --save-dev\n\u0026quot;nodemon\u0026quot;: \u0026quot;^1.18.9\u0026quot;, \u0026quot;ts-node\u0026quot;: \u0026quot;^7.0.1\u0026quot;, \u0026quot;tslint\u0026quot;: \u0026quot;^5.12.0\u0026quot;, \u0026quot;tslint-eslint-rules\u0026quot;: \u0026quot;^5.4.0\u0026quot;,  Create a tslint.json file inside root directory and copy the code given below.\n{ \u0026quot;defaultSeverity\u0026quot;: \u0026quot;error\u0026quot;, \u0026quot;extends\u0026quot;: [ \u0026quot;tslint:recommended\u0026quot;, \u0026quot;tslint-eslint-rules\u0026quot; ], \u0026quot;jsRules\u0026quot;: {}, \u0026quot;rules\u0026quot;: { \u0026quot;object-literal-shorthand\u0026quot;: false, \u0026quot;object-literal-sort-keys\u0026quot;: [ false ], \u0026quot;ter-indent\u0026quot;: [ true, 2 ], \u0026quot;no-console\u0026quot;: false, \u0026quot;only-arrow-functions\u0026quot;: [ false ], \u0026quot;member-access\u0026quot;: false, \u0026quot;max-classes-per-file\u0026quot;: [ true, 5 ], \u0026quot;no-shadowed-variable\u0026quot;: false, \u0026quot;interface-name\u0026quot;: [ false ], \u0026quot;max-line-length\u0026quot;: [ true, 200 ], \u0026quot;no-var-requires\u0026quot;: false }, \u0026quot;rulesDirectory\u0026quot;: [] }  Inside scripts in package.json copy the following code.\n\u0026quot;dev\u0026quot;: \u0026quot;nodemon --no-deprecation --watch 'src/**/*.ts' --ignore 'src/**/*.spec.ts' --exec 'ts-node' src/app.ts\u0026quot;, \u0026quot;lint\u0026quot;: \u0026quot;tslint -c tslint.json 'src/**/*.ts' --fix\u0026quot;,  We will use npm run dev for running our code in development mode and npm run lint to lint our code. Now our actual coding will begin. Create a file inside src folder named as app.ts Copy this inside src/app.ts\nfunction testFunction(): string { return \u0026quot;Messi and Ronaldo are legends\u0026quot;; } export default testFunction;  Now we will test this. Create a test folder inside src, and then create a file user-test-spec.ts. For testing we will install required modules. We will be using mocha for testing and chai for assertion. Run this command inside your project.\nnpm i --quiet chai chai-http chai-as-promised mocha mocha-typescript sinon --save-dev\nnon npm users copy this inside devDependencies.\n\u0026quot;chai\u0026quot;: \u0026quot;^4.2.0\u0026quot;, \u0026quot;chai-as-promised\u0026quot;: \u0026quot;^7.1.1\u0026quot;, \u0026quot;chai-http\u0026quot;: \u0026quot;3.0.0\u0026quot;, \u0026quot;mocha\u0026quot;: \u0026quot;^5.2.0\u0026quot;, \u0026quot;mocha-typescript\u0026quot;: \u0026quot;^1.1.17\u0026quot;, \u0026quot;sinon\u0026quot;: \u0026quot;^7.2.2\u0026quot;,  We will create a script for testing. Copy this code inside scripts\n\u0026quot;test\u0026quot;: \u0026quot;mocha --no-deprecation --timeout 10000 --require ts-node/register **/*.spec.ts\u0026quot;  We can use this command using npm test . So far our package.json will look like this.\n{ \u0026quot;name\u0026quot;: \u0026quot;graphql-todo\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;This is a GraphQL API for todo application\u0026quot;, \u0026quot;main\u0026quot;: \u0026quot;index.js\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;dev\u0026quot;: \u0026quot;nodemon --no-deprecation --watch 'src/**/*.ts' --ignore 'src/**/*.spec.ts' --exec 'ts-node' src/app.ts\u0026quot;, \u0026quot;lint\u0026quot;: \u0026quot;tslint -c tslint.json 'src/**/*.ts' --fix\u0026quot;, \u0026quot;start\u0026quot;: \u0026quot;gulp \u0026amp;\u0026amp; cd dist/ \u0026amp;\u0026amp; node --inspect=8990 --no-deprecation app.js\u0026quot;, \u0026quot;test\u0026quot;: \u0026quot;mocha --no-deprecation --timeout 10000 --require ts- node/register **/*.spec.ts\u0026quot; }, \u0026quot;keywords\u0026quot;: [], \u0026quot;author\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;ISC\u0026quot;, \u0026quot;devDependencies\u0026quot;: { \u0026quot;chai\u0026quot;: \u0026quot;^4.2.0\u0026quot;, \u0026quot;chai-as-promised\u0026quot;: \u0026quot;^7.1.1\u0026quot;, \u0026quot;chai-http\u0026quot;: \u0026quot;3.0.0\u0026quot;, \u0026quot;gulp\u0026quot;: \u0026quot;^4.0.0\u0026quot;, \u0026quot;gulp-sourcemaps\u0026quot;: \u0026quot;^2.6.4\u0026quot;, \u0026quot;gulp-typescript\u0026quot;: \u0026quot;^5.0.0\u0026quot;, \u0026quot;mocha\u0026quot;: \u0026quot;^5.2.0\u0026quot;, \u0026quot;mocha-typescript\u0026quot;: \u0026quot;^1.1.17\u0026quot;, \u0026quot;nodemon\u0026quot;: \u0026quot;^1.18.9\u0026quot;, \u0026quot;sinon\u0026quot;: \u0026quot;^7.2.2\u0026quot;, \u0026quot;ts-node\u0026quot;: \u0026quot;^7.0.1\u0026quot;, \u0026quot;tslint\u0026quot;: \u0026quot;^5.12.0\u0026quot;, \u0026quot;tslint-eslint-rules\u0026quot;: \u0026quot;^5.4.0\u0026quot;, \u0026quot;typescript\u0026quot;: \u0026quot;^3.2.2\u0026quot; } }  Inside your src/test/user-test.spec.ts copy this code\nimport chai = require(\u0026quot;chai\u0026quot;); import chaiAsPromised = require(\u0026quot;chai-as-promised\u0026quot;); import { suite, test } from \u0026quot;mocha-typescript\u0026quot;; import testFunction from \u0026quot;../app\u0026quot;; chai.use(chaiAsPromised); @suite(\u0026quot;User Test class\u0026quot;) class UserTests { @test(\u0026quot;testFunction Test - It works fine\u0026quot;) public testFunction(done) { chai.expect(testFunction()).to.deep.equal(\u0026quot;Messi and Ronaldo are legends\u0026quot;); done(); } }  Explanation:-\n  Line 1â€“4:- We are importing test modules and our file app.ts\n  Line 7â€“8:- We are defining suite to run and defining our class UserTests.\n  Line 10â€“16:- @test will declare the test message we are running, we also define a public test to run. We use chai.expect to test assetion of test.\n  Run your test by running npm test . If everything works fine, it will look like this.\nTo calculate test coverage of our code we will use nyc. We need to first check that our project is building or not. Copy this to scripts and then run npm run build\n\u0026quot;build\u0026quot;: \u0026quot;gulp\u0026quot;  You will get output as show below and a dist folder will be created.\nAdd nyc module by running npm i --quiet nyc --save-dev , non npm users copy this into package.json\n\u0026quot;nyc\u0026quot;: \u0026quot;^13.1.0\u0026quot;  Add the command given below to scripts.\n\u0026quot;coverage\u0026quot;: \u0026quot;nyc --reporter=text mocha --no-deprecation --timeout 10000 dist/test/*.spec.js -x dist/test/*.spec.js\u0026quot;,  We need to build to latest code first, then after that we will run code coverage by running npm run build \u0026amp;\u0026amp; npm run coverage. This will create a .nyc_output folder, from which our code coverage will be seen. You will see following output.\n Above image shows that we have achieved ðŸ’¯ code coverage which depicts that we have not written any unusable code.\n Docker Setup Now we will setup docker so that non npm users can also run tests and build project without installing node or npm. I assume that the readers have a basic understanding of docker. If you have never used docker before you might wanna check out this guide for getting started with Docker.\nCreate a Dockerfile inside root directory and copy this\nFROM node:8 # Install gulp and pm2 globaly RUN npm install --quiet -g gulp pm2 # Create app directory RUN mkdir -p /usr/src/ WORKDIR /usr/src/ # Install app dependencies COPY package.json /usr/src/ RUN npm install --quiet # Bundle app source COPY . /usr/src # Build the project RUN npm run build WORKDIR /usr/src/dist EXPOSE 4895 EXPOSE 5858 CMD [\u0026quot;pm2-docker\u0026quot;, \u0026quot;process.yml\u0026quot;]  Explanation:-\n  Line 1:- We are use Node image having version 8.\n  Line 4:- We globally install gulp and pm2. PM2 is a Production Runtime and Process Manager for Node.js applications with a built-in Load Balancer.\n  Line 7:- We create a new directory /usr/src/ inside our docker image.\n  Line 9:- We set /usr/src/ as our working directory inside the docker image.\n  Line 12:- We copy package.json from root directory to our working directory.\n  Line 14:- We install node modules inside docker image.\n  Line 17:- We copy rest of our code from root directory to our working directory.\n  Line 20:- We build our project using gulp. We already installed gulp inside our docker image in line 4.\n  Line 22:- We set our working directory as /usr/src/dist/ as our JS code is compiled there.\n  Line 26:- We execute pm2 and using our process.yml file that we created earlier to start the application inside docker image.\n  We need to build docker image using this Dockerfile. Add a scripts folder inside root directory and create a dockerCompose.sh file inside it. Copy the following code inside scripts/dockerCompose.sh:\n#!/usr/bin/env bash cd .. docker build -t knrt10/todoapi -f Dockerfile . docker-compose up -d  Add this inside scripts in your package.json.\n\u0026quot;dockerStart\u0026quot;: \u0026quot;cd scripts \u0026amp;\u0026amp; chmod 777 dockerCompose.sh \u0026amp;\u0026amp; ./dockerCompose.sh \u0026amp;\u0026amp; cd ..\u0026quot;, \u0026quot;dockerStop\u0026quot;: \u0026quot;docker-compose down\u0026quot;  Now that you know how to create an image with a Dockerfile, letâ€™s create an application as a service and connect it to a database. Then we can run some setup commands and be on our way to creating rest of application.\nThe Docker Compose file will define and run the containers based on a configuration file. We are using compose file version 2 syntax, and you can read about it on Dockerâ€™s site.\nAn important concept to understand is that Docker Compose spans buildtime and runtime. Up until now, we have been building images using docker build ., which is buildtime. This is when our containers are actually built. We can think of runtime as what happens once our containers are built and being used.\nCompose triggers buildtimeâ€” instructing our images and containers to build â€” but it also populates data used at runtime, such as env vars and volumes. This is important to be clear on. For instance, when we add things like volumes and command, they will override the same things that may have been set up via the Dockerfile at buildtime.\nOpen your docker-compose.yml file in your editor in root directory and copy/paste the following lines:\nversion: '3' services: web: build: . command: npm run dev volumes: - .:/usr/src/ - /usr/src/node_modules ports: - \u0026quot;3000:3000\u0026quot; depends_on: - mongodb mongodb: image: mongo ports: - \u0026quot;27017:27017\u0026quot; volumes: - ./data:/data/db  Explanation:-\nThe first directive in the web service is to build the image based on our Dockerfile. This will recreate the image we used before, but it will now be named according to the project we are in, graphql-todo. After that, we are giving the service some specific instructions on how it should operate:\n  Line 5:- Once the image is built, and the container is running, the npm start command will start the application.\n  Line 6:- volumes: â€“ This section will mount paths between the host and the container.\n  Line 7:- .:/usr/src/ â€“ This will mount the root directory to our working directory in the container.\n  Line 8:- /usr/src/node_modules â€“ This will mount the node_modules directory to the host machine using the buildtime directory.\n  Line 9:- ports: â€“ This will publish the containerâ€™s port, in this case 3000, to the host as port 3000\n  Line 11:- This depicts on what database image it depends on. We are using mongodb so we will specify mongodb\n  Line 14:- It will build mongodb image.\n  Line 15:- Same as nodejs we specify version of mongodb image here.\n  Line 17:- ports: â€“ This will publish the containerâ€™s port, in this case 27017, to the host as port 27017\n  Now before executing this script, check your docker is up and running. This command will execute our scripts/dockerCompose.sh file. This file builds our docker image. Now run npm run dockerStart\nIf everything goes right you will see this output. This means your docker image is created.\nSuccessfully built 2597b7c50ed4 Successfully tagged graphql-todo_web:latest Creating graphql-todo_mongodb_1 ... done Creating graphql-todo_web_1 ... done  You can get information about your running containers using the command given below.\ndocker ps -a\nYou will see this kind of output\nCopy the NAMES for image graphql-todo_web and run this\ndocker exec -it graphql-todo_web_1 -- /bin/bash\nThis will run bash inside the existing graphql-todo_web_1 container. The bash process will have the same Linux namespaces as the main container process. This allows you to explore the container from within and see how Node.js and your app see the system when running inside the container. The -it option is shorthand for two options:\n  -i, which makes sure STDIN is kept open. You need this for entering commands into the shell.\n  -t, which allocates a pseudo terminal (TTY).\n  Exploring container from within Now you can run same commands as other users who have node installed. Find your test coverage by running the command given below.\nnpm run coverage\nYou will see the following output.\nYou can stop docker process by running npm run dockerStop after exiting from container.\nSetting up Express server We will now setup our express server. Copy this code to your package.json and run npm i --quiet. Non npm users just copy.\n\u0026quot;dependencies\u0026quot;: { \u0026quot;@types/node\u0026quot;: \u0026quot;^10.12.15\u0026quot;, \u0026quot;bcrypt-nodejs\u0026quot;: \u0026quot;0.0.3\u0026quot;, \u0026quot;bluebird\u0026quot;: \u0026quot;^3.5.3\u0026quot;, \u0026quot;cors\u0026quot;: \u0026quot;^2.8.5\u0026quot;, \u0026quot;express\u0026quot;: \u0026quot;^4.16.4\u0026quot;, \u0026quot;helmet\u0026quot;: \u0026quot;^3.15.0\u0026quot;, \u0026quot;jsonwebtoken\u0026quot;: \u0026quot;^8.4.0\u0026quot;, \u0026quot;mongoose\u0026quot;: \u0026quot;^5.4.0\u0026quot;, \u0026quot;winston\u0026quot;: \u0026quot;2.4.0\u0026quot; }  Explanation:-\n  bcrypt-nodejs:- Its for hashing user password using gensalt and securely storing into database\n  bluebird:- For handling promises.\n  cors:- For handling Cross origin resource sharing and enabling it across all headers and domains.\n  express:- For setting up server.\n  helmet:- For securing our Express apps by setting various HTTP headers.\n  jsonwebtoken:- For creating token that will authenticate the user.\n  mongoose:- An ORM(Object-Relational Mapping) to interact with mongoDB.\n  winston:- A cool logger instead of console.log.\n  Now create a server.ts inside src folder and copy this\n\u0026quot;use strict\u0026quot;; /* Import modules */ import bluebird = require(\u0026quot;bluebird\u0026quot;); import cors = require(\u0026quot;cors\u0026quot;); import express = require(\u0026quot;express\u0026quot;); import fs = require(\u0026quot;fs\u0026quot;); import mongoose = require(\u0026quot;mongoose\u0026quot;); import { Config } from \u0026quot;./shared\u0026quot;; global.Promise = bluebird; /** * @exports TodoApp * @class * @method startServer * @method initEnv * @method initWinston * @method initExpress * @method initCORS * @method initAppRoutes * @method initServices */ export class TodoApp { public infoString: string; public port: any; private pkg = require(\u0026quot;../package.json\u0026quot;); // information about package version private winston: any = require(\u0026quot;winston\u0026quot;); // for logging private app: any; // express server constructor(private portGiven) { if (Config.dbSettings.authEnabled) { this.infoString = \u0026quot;mongodb://\u0026quot; + Config.dbSettings.username + \u0026quot;:\u0026quot; + Config.dbSettings.password + \u0026quot;@\u0026quot; + Config.dbSettings.connectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database; } else if (Config.dbSettings.localDatabase) { this.infoString = \u0026quot;mongodb://\u0026quot; + Config.dbSettings.connectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database; } else { this.infoString = \u0026quot;mongodb://\u0026quot; + Config.dbSettings.dockerconnectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database; } this.port = portGiven; } /** * This starts express server * @method startServer @public */ public startServer() { this.initEnv().then(() =\u0026gt; { // logs/ Folder already // Initilatizing the winston as per documentation this.initWinston(); this.initServices().then(() =\u0026gt; { // start the express server(s) this.initExpress(); // all done this.winston.info(this.pkg.name + \u0026quot; startup sequence completed\u0026quot;, { version: this.pkg.version, }); }); }); } /** * This setups the log folder and any other environment needs * @method initEnv @private * @returns {Promise\u0026lt;void\u0026gt;} */ private initEnv(): Promise\u0026lt;void\u0026gt; { return new Promise\u0026lt;void\u0026gt;((resolve) =\u0026gt; { const logPath: string = Config.serviceSettings.logsDir; fs.stat(logPath, (err) =\u0026gt; { resolve(); }); }); } /** * This Initilatizes the winston * @method initWinston @private */ private initWinston() { // winston is configured as a private variable to the main app.ts // it can then be spread to child modules or routeModules. This way only one winston object needs to be setup this.winston.remove(this.winston.transports.Console); this.winston.add(this.winston.transports.Console, { colorize: true, prettyPrint: true, timestamp: true, }); this.winston.add(this.winston.transports.File, { name: \u0026quot;error\u0026quot;, level: \u0026quot;error\u0026quot;, filename: \u0026quot;logs/error.log\u0026quot;, maxsize: 10485760, maxFiles: \u0026quot;10\u0026quot;, timestamp: true, }); this.winston.add(this.winston.transports.File, { name: \u0026quot;warn\u0026quot;, level: \u0026quot;warn\u0026quot;, filename: \u0026quot;logs/warn.log\u0026quot;, maxsize: 10485760, maxFiles: \u0026quot;10\u0026quot;, timestamp: true, }); this.winston.add(this.winston.transports.File, { name: \u0026quot;info\u0026quot;, level: \u0026quot;info\u0026quot;, filename: \u0026quot;logs/info.log\u0026quot;, maxsize: 10485760, maxFiles: \u0026quot;10\u0026quot;, timestamp: true, }); this.winston.add(this.winston.transports.File, { name: \u0026quot;verbose\u0026quot;, level: \u0026quot;verbose\u0026quot;, filename: \u0026quot;logs/verbose.log\u0026quot;, maxsize: 10485760, maxFiles: \u0026quot;10\u0026quot;, timestamp: true, }); this.winston.info(\u0026quot;Winston has been init\u0026quot;); } /** * This Initilatizes express server * @method initExpress @private */ private initExpress() { // create express this.app = express(); this.initCORS(); // add in any routes you might want this.initAppRoutes(); // and start! this.app.listen(this.port); this.winston.info(\u0026quot;Express started on (http://localhost:\u0026quot; + this.port + \u0026quot;/)\u0026quot;); } /** * This Initilatizes cors package * @method initCORS @private */ private initCORS() { this.app.use(cors()); } /** * This Initilatizes routes for server * @method initAppRoutes @private */ private initAppRoutes() { // We will setup our graphql route here } /** * This Initilatizes services we want if expanding the application * @method initServices @private * @returns {Promise\u0026lt;boolean\u0026gt;} */ private initServices(): Promise\u0026lt;boolean\u0026gt; { return new Promise\u0026lt;boolean\u0026gt;((resolve, reject) =\u0026gt; { // connect to mongodb mongoose.connect(this.infoString, { useNewUrlParser: true }).then(() =\u0026gt; { this.winston.info(\u0026quot;Mongo Connected!\u0026quot;); resolve(true); }); }); } }  Explanation:-\n  Line 1â€“10:- We import necessary modules.\n  Line 11:- We specify that our promises globally will be handled by bluebird.\n  Line 24-30:- We create a class TodoApp and initialize itâ€™s constructor with a public data member infostring and set it to according to what type of mongoDB we are using. If running locally authEnabled is set to false in config.ts file. If running docker localDatabase inside config.ts is set to false.\n  Line 46 :- We create a public member function to start the express server.\n  Line 70â€“77:- Here we check whether we have a logs folder or not. We are returning a promise here that we handle in line 47.\n  Line 83â€“127:- We are using winston as our logger. So here we setup winston thatâ€™s all.\n  Line 133â€“144:- We create a member function to initialize our express application.\n  Line 150â€“152:- We are intializing cors module in this member function.\n  Line 158â€“160:- In this member function we will initialize our graphQL routes in our next article.\n  Line 167â€“175:- In this member function we are returing a promise that resolves to that a mongoDB is connected. We handle this promise on line 52.\n  Now create a shared folder inside src/ and create 2 files\n  config.ts\n  index.ts\n  Copy this to src/shared/config.ts\n/** * This file stores info for api, db, keys, logs * @constant Config */ export const Config = { apiSettings: { host: process.env.API_HOST || \u0026quot;localhost\u0026quot;, }, dbSettings: { authEnabled: process.env.MONGO_AUTH || false, localDatabase: true, dockerconnectionString: process.env.MONGO_DB_HOST_DOCKER || \u0026quot;mongodb:27017\u0026quot;, connectionString: process.env.MONGO_DB_HOST || \u0026quot;localhost:27017\u0026quot;, database: process.env.DATABASE || \u0026quot;todoapp\u0026quot;, password: process.env.MONGO_AUTH_PASSWORD, username: process.env.MONGO_AUTH_USERNAME, }, serviceSettings: { logsDir: \u0026quot;logs/\u0026quot;, env: process.env.environment || \u0026quot;local\u0026quot;, }, secretKeys: { jwtSecret: process.env.SECRET || \u0026quot;yes1234$ASDASD/SA\u0026quot;, cryptoSecret: process.env.CRYPTO || \u0026quot;DASD2233312S;!`W21\u0026quot;, }, };  And this inside src/shared/index.ts\nexport * from \u0026quot;./config\u0026quot;;  Now copy this to your src/app.ts\n\u0026quot;use strict\u0026quot;; import { TodoApp } from \u0026quot;./server\u0026quot;; const server: TodoApp = new TodoApp(process.env.API_PORT || 3000); // starting the server* server.startServer();  Also create a logs folder inside project root directory and create a temp.txt file inside it. Copy the following code inside logs/temp.txt :\nThis is just an example of log file\n{\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;Winston has been init\u0026quot;,\u0026quot;timestamp\u0026quot;:\u0026quot;2018-12-26T16:59:05.380Z\u0026quot;} {\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;Mongo Connected!\u0026quot;,\u0026quot;timestamp\u0026quot;:\u0026quot;2018-12-26T16:59:05.420Z\u0026quot;} {\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;Express started on ([http://localhost:3000/](http://localhost:3000/))\u0026quot;,\u0026quot;timestamp\u0026quot;:\u0026quot;2018-12-26T16:59:05.430Z\u0026quot;} {\u0026quot;version\u0026quot;:\u0026quot;1.0.0\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;graphql-todo startup sequence completed\u0026quot;,\u0026quot;timestamp\u0026quot;:\u0026quot;2018-12-26T16:59:05.432Z\u0026quot;}  To update our tests. Copy this to src/test/user-test.spec.ts :\nimport chai = require(\u0026quot;chai\u0026quot;); import chaiAsPromised = require(\u0026quot;chai-as-promised\u0026quot;); import chaiHttp = require(\u0026quot;chai-http\u0026quot;); import { suite, test } from \u0026quot;mocha-typescript\u0026quot;; import sinon = require(\u0026quot;sinon\u0026quot;); import { TodoApp } from \u0026quot;../server\u0026quot;; import { Config } from \u0026quot;../shared\u0026quot;; // starting the server const server: TodoApp = new TodoApp(process.env.API_PORT || 3001); server.startServer(); chai.use(chaiAsPromised); chai.use(chaiHttp); @suite(\u0026quot;User Test class\u0026quot;) class UserTests { static after() { process.exit(0); } @test(\u0026quot;Testing Local Connection - try connection for Local mongodb\u0026quot;) public localDb(done) { setTimeout(() =\u0026gt; { Config.dbSettings.localDatabase = true; const mock = sinon.mock(new TodoApp(process.env.API_PORT || 3001), \u0026quot;constructor\u0026quot;); chai.expect(mock.object.infoString).to.deep.equal(\u0026quot;mongodb://\u0026quot; + Config.dbSettings.connectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database); done(); }, 100); } @test(\u0026quot;Testing Docker Connection - try connection for docker mongodb\u0026quot;) public dockerDb(done) { Config.dbSettings.localDatabase = false; const mock = sinon.mock(new TodoApp(process.env.API_PORT || 3001), \u0026quot;constructor\u0026quot;); chai.expect(mock.object.infoString).to.deep.equal(\u0026quot;mongodb://\u0026quot; + Config.dbSettings.dockerconnectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database); done(); } @test(\u0026quot;Testing Online Connection - try connection for online mongodb\u0026quot;) public OnlineDb(done) { Config.dbSettings.authEnabled = true; const mock = sinon.mock(new TodoApp(process.env.API_PORT || 3001), \u0026quot;constructor\u0026quot;); chai.expect(mock.object.infoString).to.deep.equal(\u0026quot;mongodb://\u0026quot; + Config.dbSettings.username + \u0026quot;:\u0026quot; + Config.dbSettings.password + \u0026quot;@\u0026quot; + Config.dbSettings.connectionString + \u0026quot;/\u0026quot; + Config.dbSettings.database); done(); } }  First start your mongoDB for your OS and then run npm run build \u0026amp;\u0026amp; npm run coverage. You will see the output given below.\n second set of tests with maintained ðŸ’¯ code coverage. ðŸ‘Œ\n Commiting our changes We need to commit our changes, so that if some problem occur we can roll back to this commit. First create a .gitignore file in your root directory and copy this\n*# Dependency directories* node_modules jspm_packages *# Optional npm cache directory *.npm *# Optional REPL history *.node_repl_history /bin/ /tmp/ /dist/ /typings/ /data/ /logs/*.log /.nyc_output/ /coverage/ .DS_Store package-lock.json  Now copy and run the commands given below inside your git repository. Make sure you are in root folder.\ngit add . git commit -m \u0026quot;Adding part1 changes\u0026quot;  So far your project should look like this\nConclusion That is for this part. In this part you learnt following things:-\n  How to create express server.\n  How to setup docker using dockerfile and docker-compose.yml.\n  How to use Gulp and compile code from TS to JS.\n  How to write clean code.\n  How and write tests and have ðŸ’¯ code coverage ðŸ˜ƒ.\n  In next part you will start working with graphQL and make API using that.\n  Did you find this page helpful? Consider sharing it ðŸ™Œ ","date":1546041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608897131,"objectID":"7cdc6e2ea5c8dd3618626d0db71bc429","permalink":"/post/part-1-api-using-graphql/","publishdate":"2018-12-29T00:00:00Z","relpermalink":"/post/part-1-api-using-graphql/","section":"post","summary":"This is a series of 3 articles which will help to write production grade code.","tags":["js","opensource","node.js","docker","mongodb","testing"],"title":"Part 1:- API using GraphQL and Node.js","type":"post"},{"authors":["Kautilya Tripathi"],"categories":["opensource","tech"],"content":"A little background For months my eyes were set on one goalÂ : the prestigious and famous Google Summer of Code. I had been contributing to my organization like crazy, creating PRs after PRs, raising issues and helping out fellow contributors and other aspiring GSoCers on the community\u0026rsquo;s slack team.\nThe day of the results came and out of the two slots given to my organization my project wasn\u0026rsquo;t in the list of selected projects. It was hard at first, I thought I had done everything right but still my name wasn\u0026rsquo;t up there.\nThe next morning I decided not to waste my time anymore and started to look at the bright side of things, contributing to open source project helped me learn invaluable skills like test driven development, CI/CD, git (rebasing, merge conflicts), etc. I may not be getting any stipend for writing code throughout the summer but I earned some invaluable skills which I can use in my future projects.\nSo it was exam time and I was hacking on one of my projectâ€™s as usual (a CLI tool to upload images to a cloud service) when one of my cool friends (Palash Nigam) entered my room and said with a tone of surprise â€œYou donâ€™t know how to touch type? Thatâ€™s patheticâ€ ðŸ˜€. So he introduced me to this site called typeracer (so I listened to him and started practicing on that site and I was hooked). One of my seniors from college (Shibasis Patel) gave me this idea of creating a CLI tool to play typeracer (as I had earlier asked him for some project ideas) which we could use to introduce the freshmen to both the shell and touch typing. This gave me a new purpose, so I started coding. After about a week of writing code I am proud to present to you typeracer-CLI\nWhat is typeracer-cli? So it is basically a terminal client for playing typeracer on your shell. As soon as you start the game you will be presented with a paragraph which you have to type out and at the end your time and speed (in wpm) are recorded and presented as an output.\n Whatâ€™s new about this? CLI versions of this game already exist. Agreed, other versions of this already exists but they donâ€™t offer all the features that this client does, like:\n  Practice mode (offline mode) User stats (words per minute, time taken) Online mode (have a type-race by spawning up a server and sharing it with your friends) Ask for a rematch after the race ends (online mode) View the top 10 High scores in online mode  The motivation behind it Well, they say failure is a great motivator. I learned this the hard way. Failing in getting selected for GSoC motivated me even more to be a better developer. The other big reason was the desire to do something for my college. I had realized the benefits of working in a developer community during the time I was preparing for GSoC. Although I was a part of my collegeâ€™s Programming Society I hadnâ€™t contributed as actively as I should have. This project turned out to be one of the ways of contributing to my community by spreading awareness about touch typing and CLI tools among young aspiring developers who are just starting out.\nImplementation Initially the task was to get keystrokes from the userâ€™s terminal which at that time I thought was impossible. But I found about readline and keyspress events in nodejs which helped me to move further in coding.\nThe tasks were broken up into the following:\n Conver this tool to an npm package Offline practice mode Generate random paragraph for every race Add more sensible paragraphs Display the userâ€™s time and speed as they type Setup server for online mode Improve the API Design Write tests  Getting into every point in detail Converting it to an npm package This was important task so that one can easily download the package and install it globally from npm. For that we need to use a very important line on the start of the file that is going to execute.\n #!/usr/bin/env node is an instance of a shebang line: the very first line in an executable plain-text file on Unix-like platforms that tells the system what interpreter to pass that file to for execution, via the command line following the magic #! prefix (called shebang)\n Although Windows does not support shebang lines, so theyâ€™re effectively ignored there; on Windows it is solely a given fileâ€™s filename extension that determines what executable will interpret it. However, you still need them in the context of npm.\nImplementing the offline (practice) mode Initially some commands were written for the execution of practice mode. With the help of a package commander I was able to achieve this task.\nprogram .command('practice') .alias('p') .description('Starts typeracer in practice mode') .action(() =\u0026gt; { game() })  game() function This is main logic that allows the application to get keystrokes from the client, but we also have to listen to keypress event for completion of this task. stdin.on('keypress', keypress)\nconst stdin = process.stdin const stdout = process.stdout stdin.setRawMode(true) stdin.resume() require('readline').emitKeypressEvents(stdin)  Now in game() I was enabling keypress event after 5 seconds of game, and showing paragraphs to user so that they get time to relax their fingers, twist turn their neck, crack their knuckles and say â€œbring it onâ€.\nI was displaying three things to client when they were typing\n Real time analysis of their typing with green, red representing correct and wrong characters respectively.  /** * @function color * @param {String} quote * @param {String} stringTyped */ function color (quote, stringTyped) { let colouredString = '' let wrongInput = false const quoteLetters = quote.split('') const typedLetters = stringTyped.split('') for (let i = 0; i \u0026lt; typedLetters.length; i++) { // if a single mistake, // the rest of the coloured string will appear red if (wrongInput) { colouredString += chalk.bgRed(quoteLetters[i]) continue } if (typedLetters[i] === quoteLetters[i]) { wrongInput = false colouredString += chalk.green(quoteLetters[i]) if (quote === stringTyped) { gameEnd = true } } else { wrongInput = true colouredString += chalk.bgRed(quoteLetters[i]) } } return colouredString }   Real time analysis of their speed in words per minute.  The following snippet explains how to get the speed of a user according to correct words typed by them.\n/** * @function updateWpm */ function updateWpm () { if (stringTyped.length \u0026gt; 0) { wordsPermin = stringTyped.split(' ').length / (time / 60) } }   Calculating the time taken  /** * @function Time */ function Time () { time = (Date.now() - timeStarted) / 1000 }  In the end there is an option to retry where you can restart the match with generation of new paragraph every time.\nOnline Mode This was very important feature to implement as this sets this client apart from other CLI versions. This was implemented using socket.io accordingly:\n Creating a server Connecting clients to the server Create private room for competition Send scores to all clients at end of game Rematch feature Random paragraphs for every race Top 10 high scores  Getting into every point in detail Creating a server I am quite fluent with Javascript and NodeJs, so I used NodeJs for creating server of the application and hosted in on Glitch.\nSocket.io was used to provide web sockets for clients to connect and emit events for server to listen. MongoDB was used as database for storing top 10 high scores of clients.\nConnecting clients to sever Initially the client part was quite tricky as I had not understood socket.io upto basic level. At first I was working on local server or you can localhost. I was using socket.io client for client side but still took me a whole day to understand the basic and connect a client to the server.\nCreating a private room for competition Now in socket.io you can create different namespace or you can say rooms to join. So I had to get some input from user to create a private channel where they can race otherwise it would create countless problems. I used the crypto node module to provide cryptographic functionality that includes a set of wrappers for OpenSSLâ€™s hash, HMAC, cipher, decipher, sign, and verify functions and generated random strings with it every time.\nconst roomNumber = crypto .randomBytes(12) .toString('base64') .replace(/[+/=]+/g, '')  Another problem was how the server would know the number of players in a room so that it can emit an event for race to start. For that I asked number of players from client they wanted to race with (using the npm package inquirer.)\nSo when the user joined the room I emitted all the information of client to sever so that it can work according to that.\n// Emitting client info on joining the room _socket.on('room', function (val) { _socket.emit('join', {roomName: val.value, username: data.username, number: data.number, randomNumber: data.randomNumber}) })  Now when the server knew that all the clients have joined the race it emitted an event to clients and started the race. It was important to send random paragraphs on every connection and also same paragraph to all clients in a same room.\n/** * @function randomNumRetry */ function randomNumRetry () { randomNumber = Math.floor((Math.random() * paras.length)) quote = paras[randomNumber].para if (quote.length \u0026lt; 100) { quote = paras[randomNumber].para + ' ' + paras[randomNumber - 1].para } return quote }  When everything was in order and every client completed the race, the server emitted an event sending all the scores to every client and asking if they wanted a rematch. Similarly for a rematch random paragraph was generated.\nTop 10 high scores For this feature to work I initially created a shell database with ten anonymous users with their scores initialized to 0. Now whenever someone plays an online game and score greater than 10th highest score in database, it replaces the 10th highest scorer with the user in the database (this was done to avoid excessive use of database).\n// Getting documents from databse Score.findOne({_id: process.env.ID}, (err, players) =\u0026gt; { if (err) throw new Error(err) let playersArray = players.players.sort(function (a, b) { return b.score - a.score }) let lowestScore = [] lowestScore.push(playersArray[playersArray.length - 1].score) // checking if last score is less then current score function remove () { // First removing last player Score.update({_id: process.env.ID}, {$pop: {players: 1}}, (err) =\u0026gt; { if (err) throw new Error(err) console.log('Removed last player') }) } function add () { // Then updating current player Score.update({_id: process.env.ID}, {$push: {players: {score, username}}}, (err) =\u0026gt; { if (err) throw new Error(err) console.log('Added new High score') }) } async function update () { // Then again sorting it correctly await Score.update({_id: process.env.ID}, {$push: {players: {$each: [], $sort: -1}}}, (err) =\u0026gt; { if (err) throw new Error(err) console.log('Sorted in descending order after adding') }) } if (score \u0026gt; lowestScore[0]) { (async () =\u0026gt; { Promise.all([update()]).then(async () =\u0026gt; { await remove() await add() await update() }) })() } })  Support Us This project was a great learning experience for me and we (my friends and I) are looking to build more such awesome projects in the future. We are a bunch of undergrads passionate about software development looking to make cool stuff. A little motivation and support helps us a lot. If you like this nifty hack you can support us by doing any (or all ðŸ˜‰ ) of the following:\n â­ï¸ Star us on Github and make it trend so that other people can know about our project. Install it and increase our download count on npm. Tweet about it (our handle is psociiit).  Thanks to Palash Nigam for helping me to write this article and also Shibasis Patel for sharing this cool idea.\nDid you find this page helpful? Consider sharing it ðŸ™Œ ","date":1526256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608897131,"objectID":"61351d3ce18d88d2104d237518b8cb87","permalink":"/post/typeracer/","publishdate":"2018-05-14T00:00:00Z","relpermalink":"/post/typeracer/","section":"post","summary":"Why I created a typeracing application.","tags":["js","opensource","node.js","touchtyping"],"title":"ShellRacer","type":"post"}]